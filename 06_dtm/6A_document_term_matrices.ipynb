{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Term Matrices\n",
    "\n",
    "So far we've been making our own pandas dataframes of custom data. This is great, but there's also a very specific type of dataframe that helps us do a lot of different things. This is called a document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a document-term matrix?\n",
    "\n",
    "A document-term matrix (DTM) is simply a dataframe with terms (words) as columns, documents (texts) as rows, and the cells are the counts of those terms in those documents. For instance, if we had three documents:\n",
    "\n",
    "* D1 = \"I like this class\"\n",
    "* D2 = \"I love this class\"\n",
    "* D3 = \"I tolerate this class\"\n",
    "\n",
    "Then the document-term matrix would be:\n",
    "\n",
    "| # |I|like|love|tolerate|this|class|\n",
    "|--|-|----|----|--------|----|-----|\n",
    "|D1|1|1   |0   |0       |1   |1    |\n",
    "|D2|1|0   |1   |0       |1   |1    |\n",
    "|D3|1|0   |0   |1       |1   |1    |\n",
    "\n",
    "Why are these useful? With this type of dataframe, we can more easily:\n",
    "\n",
    "* find the words which distinguish one group of texts from another group of texts.\n",
    "* find which words often appear together in documents.\n",
    "* calculate the \"distance\" between texts in terms of their word usage.\n",
    "* cluster texts based on their word usage.\n",
    "* ...and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some things\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for an exercise below\n",
    "letters = ['a','b','c','d','e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing how to make a dataframe (from lists of dictionaries)\n",
    "\n",
    "We've been using a routine to make dataframes which we should maybe spell out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Make a new list (for all result dictionaries)\n",
    "\n",
    "# (2) Loop over something\n",
    "    \n",
    "    # (3) For each thing in the loop, make a dictionary\n",
    "    \n",
    "    # (4) Add some things to the dictionary\n",
    "    \n",
    "    # (5) add the individual result dictionary to the list of result dictionaries\n",
    "    \n",
    "# (6) make a dataframe from the list of dictionaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @TODO: Make a dataframe that looks like this:\n",
    "\n",
    "| name | status |\n",
    "|------|--------|\n",
    "|Rosencrantz|dead|\n",
    "|Guildenstern|dead|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do so, loop over this list\n",
    "names = ['Rosencrantz', 'Guildenstern']\n",
    "\n",
    "# 1) Make a new list (for all result dictionaries)\n",
    "\n",
    "# (2) Loop over something\n",
    "    \n",
    "    # (3) For each thing in the loop, make a dictionary\n",
    "    \n",
    "    # (4) Add some things to the dictionary\n",
    "    \n",
    "    # (5) add the individual result dictionary to the list of result dictionaries\n",
    "    \n",
    "# (6) make a dataframe from the list of dictionaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing how to loop over files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Loop over metadata column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "df_meta = pd.read_excel('../corpora/harry_potter/metadata.xls')\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the filename column:\n",
    "df_meta.fn\n",
    "#\n",
    "# (or)\n",
    "#\n",
    "df_meta['fn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Finish this:\n",
    "\n",
    "# 1) Set a folder for this corpus\n",
    "text_folder = '../corpora/harry_potter/texts/'\n",
    "\n",
    "# 2) Loop over the filename column\n",
    "\n",
    "    \n",
    "    # 3) Get and print the full path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Loop over the text files\n",
    "\n",
    "We can also loop over files in a text folder directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames in a folder\n",
    "filenames = os.listdir(text_folder)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Finish this:\n",
    "\n",
    "# 1) Set a folder for this corpus\n",
    "text_folder = '../corpora/harry_potter/texts/'\n",
    "\n",
    "# 2) Loop over the filename list\n",
    "\n",
    "    # 3) Get and print the full path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Sometimes we have to check if the file is a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filename endswith .txt\n",
    "example_filename = 'The Bible.txt'\n",
    "example_filename.endswith('.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filename ends with .txt?\n",
    "example_filename = 'The Bible.jesus'\n",
    "example_filename.endswith('.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick detour: Stopwords!\n",
    "\n",
    "What's a stopword? A word we don't want to count! Function words, pronouns, common verbs and adverbs. Stop word lists are highly variable. NLTK gives us one. We need to download it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords list\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list=stopwords.words('english')\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets let us check whether a word is in the stopword list faster\n",
    "stopword_set = set(stopword_list)\n",
    "\n",
    "# is 'us' in the stopwords?\n",
    "'us' in stopword_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is 'you' in the stopwords?\n",
    "'you' in stopword_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make a document-term matrix\n",
    "\n",
    "For this notebook, we'll be working with the 118 State of the Union speeches given by U.S. Presidents from 1900 to 2018. You can [download this corpus here](https://www.dropbox.com/sh/xd854hgyvbysqlm/AAAhbS6r7MFe4SVg1BFuuMTCa?dl=1). Please unzip it to your \"corpora\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set text folder and metadata path\n",
    "# (If you don't have this corpus, please download it here): https://www.dropbox.com/sh/xd854hgyvbysqlm/AAAhbS6r7MFe4SVg1BFuuMTCa?dl=1\n",
    "\n",
    "text_folder = '../corpora/sotu_1900-2018/texts'\n",
    "path_to_metadata='../corpora/sotu_1900-2018/metadata.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major step 1: Make a list of dictionaries (of counts per text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) make a counter for a corpus-wide word count\n",
    "from collections import Counter\n",
    "all_counts = Counter()\n",
    "\n",
    "# 1) make an empty results list\n",
    "all_results = []\n",
    "\n",
    "# 2) Loop over the filenames\n",
    "filenames=sorted(os.listdir(text_folder))\n",
    "for i,fn in enumerate(filenames):\n",
    "    \n",
    "    # make sure filename is a text file\n",
    "    if not fn.endswith('.txt'): continue\n",
    "    \n",
    "    # just for a progress report:\n",
    "    if not i%10:   #if i not divisible by 10\n",
    "        # print some progress\n",
    "        print('>> looping through #',i,'of',len(filenames),'files:',fn)\n",
    "    \n",
    "    # 3) get full path\n",
    "    full_path = os.path.join(text_folder,fn)\n",
    "\n",
    "    # 4) open the file\n",
    "    with open(full_path) as file:\n",
    "        txt=file.read()\n",
    "\n",
    "    # 5) make a blob\n",
    "    blob = TextBlob(txt.lower())\n",
    "\n",
    "    # 6) make a result dictionary\n",
    "    text_result = {}\n",
    "\n",
    "    # 7) set the filename\n",
    "    text_result['fn']=fn\n",
    "\n",
    "    # 8) get the number of words\n",
    "    num_words = len(blob.words)\n",
    "\n",
    "    # 9) for each word,count pair in the blob.word_counts dictionary...\n",
    "    for word,count in blob.word_counts.items():\n",
    "        \n",
    "        # is the word in the stopwords? if so, keep going\n",
    "        if word in stopword_set: continue  \n",
    "\n",
    "        # is the word a punctuation? if so, keep going\n",
    "        if not word[0].isalpha(): continue\n",
    "\n",
    "        \n",
    "        # 10) set the normalized count for this word to the text_result dictionary\n",
    "        text_result[word] = count / num_words\n",
    "            \n",
    "        # 11) add the count to the dictionary of counts for all words\n",
    "        all_counts[word]+=count\n",
    "\n",
    "    # 12) add result dictionary to all_results\n",
    "    all_results.append(text_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here's what the first dictionary looks like in our list of dictionaries\n",
    "#all_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major step #2: Convert list of dictionaries to a dataframe\n",
    "\n",
    "There are many many kinds of words in these texts. Here's the number of unique words in all our all_counts dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's too many columns for our document-term matrix! So let's get the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because all_counts is a Counter (see above), we can find the most common 10 words\n",
    "all_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's convert the results to a dataframe *while also limiting the number of columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Convert all_results to dataframe while limiting number of columns\n",
    "#\n",
    "\n",
    "# set number of words we want\n",
    "n_top_words = 1000\n",
    "\n",
    "# 13) Get the most frequent words\n",
    "most_common_words_plus_counts = all_counts.most_common(n_top_words)\n",
    "\n",
    "# 14) Get only the words\n",
    "words_we_want = []\n",
    "for word,count in most_common_words_plus_counts:\n",
    "    words_we_want.append(word)\n",
    "\n",
    "# 15) set a list for the columns, which is the words we want plus the 'fn' column\n",
    "columns = words_we_want\n",
    "columns.append('fn')\n",
    "\n",
    "# 16) Make dataframe\n",
    "dtm = pd.DataFrame(all_results, columns=columns)\n",
    "\n",
    "# 17) Set the filename as the index and fill empty values with 0\n",
    "dtm=dtm.set_index('fn').fillna(0)\n",
    "\n",
    "# show!\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can sort by particular words\n",
    "dtm.sort_values('poverty',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can sort by particular words\n",
    "dtm.sort_values('jobs',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DTMs with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for this corpus\n",
    "df_meta = pd.read_excel(path_to_metadata).set_index('fn')\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "dtm_meta=df_meta.merge(dtm,on='fn')\n",
    "dtm_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting meta+DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot poverty over time\n",
    "dtm_meta.plot(x='Year',y='poverty',figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-line graphs\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# The Long History of America First?\n",
    "dtm_meta.plot(x='Year',y='america',ax=ax)\n",
    "dtm_meta.plot(x='Year',y='world',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots by party\n",
    "\n",
    "dtm_meta.boxplot('world',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poverty?\n",
    "dtm_meta.boxplot('poverty',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immigration?\n",
    "dtm_meta.boxplot('immigration',by='Party',figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
