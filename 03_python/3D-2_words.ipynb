{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3D) Lists and words\n",
    "## (3D-2) Words\n",
    "\n",
    "In this notebook, we'll learn:\n",
    "\n",
    "* How to split texts (strings) into **words** (lists)\n",
    "* How to use the **Natural Language Toolkit (NLTK)**, a Python module\n",
    "* How to calculate **Type-Token Ratio**\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we move from strings of characters to lists of words? How do we move from\n",
    "\n",
    "```python\n",
    "text_as_string = \"Perhaps people thought that doom could be pushed forward and away...\"\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "text_as_list = ['Perhaps', 'people', 'thought', 'that', 'doom', 'could', 'be', 'pushed', 'forward', 'and', 'away', '...']\n",
    "```\n",
    "\n",
    "This process is called **tokenization** and is a major part of all natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `string.split()`\n",
    "\n",
    "The simplest way to tokenize a string is to use a built-in method of strings called `split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text_as_string and turn it into a list\n",
    "text_as_string = \"      Perhaps people thought that doom could be pushed forward and away ...   \"\n",
    "\n",
    "# Split!\n",
    "text_as_list = text_as_string.split()\n",
    "\n",
    "# print\n",
    "print(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a new way of counting words!\n",
    "\n",
    "# print the length of the STRING (measured in CHARACTERS)\n",
    "print('Length of text_as_string:', len(text_as_string) )\n",
    "\n",
    "# print the length of the LIST (measured in \"ELEMENTS\" [here, words])\n",
    "print('Length of text_as_list:', len(text_as_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO:\n",
    "# Split Bobby's first paragraph (from his second chapter [chapter 12])\n",
    "# Then count the number of words\n",
    "#\n",
    "\n",
    "bobby=\"\"\"Letter came today. Go figure. Never heard of this cousin, but he’s got it all right: name of Bobby’s dad, name of his mom, name of his uncles. Bobby reads it again. Reads it three times. Cousin from Hong Tian in Fujian, same village as his mother’s father. Gotta be a distant cousin. Could be a trick, but could be legit. Besides, how’d he find him? How’d he know Bobby was Chinese from Singapore? Knows everything. Knows Bobby’s Chinese name. Knows about the family bicycle business. Cousin’s in trouble. Musta got smuggled in. One of those boat people. Most never make it. This one might not either. All he’s got is Bobby’s name and address. Now the smugglers want their money. But where’s the cousin? Tijuana. Just turn over the money. Five thou to get the cousin across. If he makes it, five thou to get him free. China to Chinatown. That’s the deal.\"\"\"\n",
    "\n",
    "bobby_list = bobby.split()\n",
    "\n",
    "print(bobby_list)\n",
    "len(bobby_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO:\n",
    "# Print the first three words from Bobby's paragraph [using the list you made]\n",
    "# Print the last three words from Bobby's paragraph [using the list you made]\n",
    "#\n",
    "\n",
    "print(bobby_list[:3])\n",
    "print(bobby_list[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing our own tokenizer\n",
    "\n",
    "Let's write our own tokenizer—as a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('--',' -- ')\n",
    "    string = string.replace('?',' ? ')\n",
    "    string = string.replace('!',' ! ')\n",
    "    \n",
    "    \n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_punct(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens,\n",
    "    WITHOUT PUNCTUATION.\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('--',' ')\n",
    "    string = string.replace('?','')\n",
    "    string = string.replace('!','')\n",
    "    string = string.replace('.','')\n",
    "    \n",
    "    \n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize2(string):\n",
    "#     tokens = string.split()\n",
    "#     for token in tokens:\n",
    "#         if not token[-1].isalpha():\n",
    "#             token=token[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tokenize_without_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str='How many words--words I say!--are in this string?'\n",
    "\n",
    "tokenize_without_punct(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_without_punct(bobby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Load one of your texts and tokenize it\n",
    "#\n",
    "\n",
    "!pwd\n",
    "\n",
    "with open('../corpora/my_corpus/texts/ryans_diss_chap3.txt') as file:\n",
    "    my_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokens = tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's tokenizer\n",
    "\n",
    "Writing tokenizers is tricky! We don't have to reinvent the wheel. A great source for natural language processing (NLP) functions, written in Python, is [NLTK](http://www.nltk.org/). There's also a [free online book](http://www.nltk.org/book/) teaching you NLP in Python with NLTK—check that out for more information and a ton of helpful tutorials and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK. If this doesn't work, open terminal and type: pip install nltk\n",
    "import nltk\n",
    "\n",
    "test_str = \" Hi. I would like 2 bananas. 40$ heuser@emaier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK still misses a few things:\n",
    "\n",
    "nltk.word_tokenize('A word—any word—is a whole world.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could improve NLTK's tokenizer\n",
    "\n",
    "def tokenize2(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens.\n",
    "    \n",
    "    It will use NLTK's word_tokenize() function as its main process.\n",
    "    But it will also clean up the string *before* passing it to word_tokenize().\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('—', ' — ')\n",
    "    return nltk.word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize2(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize2('A word—any word—is a whole world.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type/Token Ratio\n",
    "\n",
    "A common statistic in corpus linguistics/text mining is the type token ratio (TTR). It is defined simply by\n",
    "\n",
    "    TTR = (# of word types) / (# of word tokens)\n",
    "    \n",
    "Where\n",
    "\n",
    "* \\# of word **types** = the number of *unique* words in a list of words\n",
    "* \\# of word **tokens** = the number of *all* words, including repeated words; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Get the number of tokens for Bobby's paragraph\n",
    "#\n",
    "\n",
    "num_tokens_bobby = len(bobby_list)\n",
    "num_tokens_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the types, we use the SET data type\n",
    "\n",
    "the_question_as_list = ['to','be','or','not','to','be']\n",
    "the_question_as_set = set(the_question_as_list)\n",
    "\n",
    "print('Hamlet as a list:',the_question_as_list)\n",
    "print('Hamlet as a SET:', the_question_as_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Get the number of types for Bobby's paragraph\n",
    "#\n",
    "\n",
    "bobby_set = set(bobby_list)\n",
    "\n",
    "num_types_bobby = len(bobby_set)\n",
    "num_types_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Print the TTR for Bobby's paragraph\n",
    "#\n",
    "\n",
    "ttr_bobby = num_types_bobby / num_tokens_bobby\n",
    "ttr_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Write a function to calculate the TTR from any *STRING*\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NLTK Goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Rafaela's chapter\n",
    "\n",
    "with open('../corpora/tropic_of_orange/texts/ch01.txt') as file:\n",
    "    rafaela_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Rafaela\n",
    "\n",
    "rafaela_tokens = nltk.word_tokenize(rafaela_str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an NLTK text object\n",
    "\n",
    "rafaela_nltk = nltk.Text(rafaela_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rafaela_nltk.concordance('crab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rafaela_nltk.concordance('orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO:\n",
    "# - Load one of your texts\n",
    "# - Make a concordance for a word of interest\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this line if the figure doesn't show up the first time\n",
    "\n",
    "rafaela_nltk.dispersion_plot(['crab','iguana','orange','tropic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO:\n",
    "# - Load one of your texts\n",
    "# - Make a dispersion plot for a few words of interest\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
