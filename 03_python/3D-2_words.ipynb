{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3D) Lists and words\n",
    "## (3D-2) Words\n",
    "\n",
    "In this notebook, we'll learn:\n",
    "\n",
    "* How to split texts (strings) into **words** (lists)\n",
    "* How to use the **Natural Language Toolkit (NLTK)**, a Python module\n",
    "* How to calculate **Type-Token Ratio**\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we move from strings of characters to lists of words? How do we move from\n",
    "\n",
    "```python\n",
    "text_as_string = \"Perhaps people thought that doom could be pushed forward and away...\"\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "text_as_list = ['Perhaps', 'people', 'thought', 'that', 'doom', 'could', 'be', 'pushed', 'forward', 'and', 'away', '...']\n",
    "```\n",
    "\n",
    "This process is called **tokenization** and is a major part of all natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `string.split()`\n",
    "\n",
    "The simplest way to tokenize a string is to use a built-in method of strings called `split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Perhaps', 'people', 'thought', 'that', 'doom', 'could', 'be', 'pushed', 'forward', 'and', 'away', '...']\n"
     ]
    }
   ],
   "source": [
    "# Split text_as_string and turn it into a list\n",
    "text_as_string = \"      Perhaps people thought that doom could be pushed forward and away ...   \"\n",
    "\n",
    "# Split!\n",
    "text_as_list = text_as_string.split()\n",
    "\n",
    "# print\n",
    "print(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text_as_string: 78\n",
      "Length of text_as_list: 12\n"
     ]
    }
   ],
   "source": [
    "# We now have a new way of counting words!\n",
    "\n",
    "# print the length of the STRING (measured in CHARACTERS)\n",
    "print('Length of text_as_string:', len(text_as_string) )\n",
    "\n",
    "# print the length of the LIST (measured in \"ELEMENTS\" [here, words])\n",
    "print('Length of text_as_list:', len(text_as_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Letter', 'came', 'today.', 'Go', 'figure.', 'Never', 'heard', 'of', 'this', 'cousin,', 'but', 'he’s', 'got', 'it', 'all', 'right:', 'name', 'of', 'Bobby’s', 'dad,', 'name', 'of', 'his', 'mom,', 'name', 'of', 'his', 'uncles.', 'Bobby', 'reads', 'it', 'again.', 'Reads', 'it', 'three', 'times.', 'Cousin', 'from', 'Hong', 'Tian', 'in', 'Fujian,', 'same', 'village', 'as', 'his', 'mother’s', 'father.', 'Gotta', 'be', 'a', 'distant', 'cousin.', 'Could', 'be', 'a', 'trick,', 'but', 'could', 'be', 'legit.', 'Besides,', 'how’d', 'he', 'find', 'him?', 'How’d', 'he', 'know', 'Bobby', 'was', 'Chinese', 'from', 'Singapore?', 'Knows', 'everything.', 'Knows', 'Bobby’s', 'Chinese', 'name.', 'Knows', 'about', 'the', 'family', 'bicycle', 'business.', 'Cousin’s', 'in', 'trouble.', 'Musta', 'got', 'smuggled', 'in.', 'One', 'of', 'those', 'boat', 'people.', 'Most', 'never', 'make', 'it.', 'This', 'one', 'might', 'not', 'either.', 'All', 'he’s', 'got', 'is', 'Bobby’s', 'name', 'and', 'address.', 'Now', 'the', 'smugglers', 'want', 'their', 'money.', 'But', 'where’s', 'the', 'cousin?', 'Tijuana.', 'Just', 'turn', 'over', 'the', 'money.', 'Five', 'thou', 'to', 'get', 'the', 'cousin', 'across.', 'If', 'he', 'makes', 'it,', 'five', 'thou', 'to', 'get', 'him', 'free.', 'China', 'to', 'Chinatown.', 'That’s', 'the', 'deal.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## @TODO:\n",
    "# Split Bobby's first paragraph (from his second chapter [chapter 12])\n",
    "# Then count the number of words\n",
    "#\n",
    "\n",
    "bobby=\"\"\"Letter came today. Go figure. Never heard of this cousin, but he’s got it all right: name of Bobby’s dad, name of his mom, name of his uncles. Bobby reads it again. Reads it three times. Cousin from Hong Tian in Fujian, same village as his mother’s father. Gotta be a distant cousin. Could be a trick, but could be legit. Besides, how’d he find him? How’d he know Bobby was Chinese from Singapore? Knows everything. Knows Bobby’s Chinese name. Knows about the family bicycle business. Cousin’s in trouble. Musta got smuggled in. One of those boat people. Most never make it. This one might not either. All he’s got is Bobby’s name and address. Now the smugglers want their money. But where’s the cousin? Tijuana. Just turn over the money. Five thou to get the cousin across. If he makes it, five thou to get him free. China to Chinatown. That’s the deal.\"\"\"\n",
    "\n",
    "bobby_list = bobby.split()\n",
    "\n",
    "print(bobby_list)\n",
    "len(bobby_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Letter', 'came', 'today.']\n",
      "['That’s', 'the', 'deal.']\n"
     ]
    }
   ],
   "source": [
    "# @TODO:\n",
    "# Print the first three words from Bobby's paragraph [using the list you made]\n",
    "# Print the last three words from Bobby's paragraph [using the list you made]\n",
    "#\n",
    "\n",
    "print(bobby_list[:3])\n",
    "print(bobby_list[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing our own tokenizer\n",
    "\n",
    "Let's write our own tokenizer—as a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('--',' -- ')\n",
    "    string = string.replace('?',' ? ')\n",
    "    string = string.replace('!',' ! ')\n",
    "    \n",
    "    \n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_punct(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens,\n",
    "    WITHOUT PUNCTUATION.\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('--',' ')\n",
    "    string = string.replace('?','')\n",
    "    string = string.replace('!','')\n",
    "    string = string.replace('.','')\n",
    "    \n",
    "    \n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize2(string):\n",
    "#     tokens = string.split()\n",
    "#     for token in tokens:\n",
    "#         if not token[-1].isalpha():\n",
    "#             token=token[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tokenize_without_punct in module __main__:\n",
      "\n",
      "tokenize_without_punct(string)\n",
      "    This function will accept a string as an argument,\n",
      "    and return a list of its tokens,\n",
      "    WITHOUT PUNCTUATION.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenize_without_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'many', 'words', 'words', 'I', 'say', 'are', 'in', 'this', 'string']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str='How many words--words I say!--are in this string?'\n",
    "\n",
    "tokenize_without_punct(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Letter',\n",
       " 'came',\n",
       " 'today',\n",
       " 'Go',\n",
       " 'figure',\n",
       " 'Never',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'this',\n",
       " 'cousin,',\n",
       " 'but',\n",
       " 'he’s',\n",
       " 'got',\n",
       " 'it',\n",
       " 'all',\n",
       " 'right:',\n",
       " 'name',\n",
       " 'of',\n",
       " 'Bobby’s',\n",
       " 'dad,',\n",
       " 'name',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mom,',\n",
       " 'name',\n",
       " 'of',\n",
       " 'his',\n",
       " 'uncles',\n",
       " 'Bobby',\n",
       " 'reads',\n",
       " 'it',\n",
       " 'again',\n",
       " 'Reads',\n",
       " 'it',\n",
       " 'three',\n",
       " 'times',\n",
       " 'Cousin',\n",
       " 'from',\n",
       " 'Hong',\n",
       " 'Tian',\n",
       " 'in',\n",
       " 'Fujian,',\n",
       " 'same',\n",
       " 'village',\n",
       " 'as',\n",
       " 'his',\n",
       " 'mother’s',\n",
       " 'father',\n",
       " 'Gotta',\n",
       " 'be',\n",
       " 'a',\n",
       " 'distant',\n",
       " 'cousin',\n",
       " 'Could',\n",
       " 'be',\n",
       " 'a',\n",
       " 'trick,',\n",
       " 'but',\n",
       " 'could',\n",
       " 'be',\n",
       " 'legit',\n",
       " 'Besides,',\n",
       " 'how’d',\n",
       " 'he',\n",
       " 'find',\n",
       " 'him',\n",
       " 'How’d',\n",
       " 'he',\n",
       " 'know',\n",
       " 'Bobby',\n",
       " 'was',\n",
       " 'Chinese',\n",
       " 'from',\n",
       " 'Singapore',\n",
       " 'Knows',\n",
       " 'everything',\n",
       " 'Knows',\n",
       " 'Bobby’s',\n",
       " 'Chinese',\n",
       " 'name',\n",
       " 'Knows',\n",
       " 'about',\n",
       " 'the',\n",
       " 'family',\n",
       " 'bicycle',\n",
       " 'business',\n",
       " 'Cousin’s',\n",
       " 'in',\n",
       " 'trouble',\n",
       " 'Musta',\n",
       " 'got',\n",
       " 'smuggled',\n",
       " 'in',\n",
       " 'One',\n",
       " 'of',\n",
       " 'those',\n",
       " 'boat',\n",
       " 'people',\n",
       " 'Most',\n",
       " 'never',\n",
       " 'make',\n",
       " 'it',\n",
       " 'This',\n",
       " 'one',\n",
       " 'might',\n",
       " 'not',\n",
       " 'either',\n",
       " 'All',\n",
       " 'he’s',\n",
       " 'got',\n",
       " 'is',\n",
       " 'Bobby’s',\n",
       " 'name',\n",
       " 'and',\n",
       " 'address',\n",
       " 'Now',\n",
       " 'the',\n",
       " 'smugglers',\n",
       " 'want',\n",
       " 'their',\n",
       " 'money',\n",
       " 'But',\n",
       " 'where’s',\n",
       " 'the',\n",
       " 'cousin',\n",
       " 'Tijuana',\n",
       " 'Just',\n",
       " 'turn',\n",
       " 'over',\n",
       " 'the',\n",
       " 'money',\n",
       " 'Five',\n",
       " 'thou',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'cousin',\n",
       " 'across',\n",
       " 'If',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'it,',\n",
       " 'five',\n",
       " 'thou',\n",
       " 'to',\n",
       " 'get',\n",
       " 'him',\n",
       " 'free',\n",
       " 'China',\n",
       " 'to',\n",
       " 'Chinatown',\n",
       " 'That’s',\n",
       " 'the',\n",
       " 'deal']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_without_punct(bobby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryan/Dropbox/PHD/Teaching/Literary Text Mining/literarytextmining/03_python\n"
     ]
    }
   ],
   "source": [
    "## @TODO: Load one of your texts and tokenize it\n",
    "#\n",
    "\n",
    "!pwd\n",
    "\n",
    "with open('../corpora/my_corpus/texts/ryans_diss_chap3.txt') as file:\n",
    "    my_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokens = tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's tokenizer\n",
    "\n",
    "Writing tokenizers is tricky! We don't have to reinvent the wheel. A great source for natural language processing (NLP) functions, written in Python, is [NLTK](http://www.nltk.org/). There's also a [free online book](http://www.nltk.org/book/) teaching you NLP in Python with NLTK—check that out for more information and a ton of helpful tutorials and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK. If this doesn't work, open terminal and type: pip install nltk\n",
    "import nltk\n",
    "\n",
    "test_str = \" Hi. I would like 2 bananas. 40$ heuser@emaier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " '.',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " '2',\n",
       " 'bananas',\n",
       " '.',\n",
       " '40',\n",
       " '$',\n",
       " 'heuser',\n",
       " '@',\n",
       " 'emaier']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'word—any', 'word—is', 'a', 'whole', 'world', '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK still misses a few things:\n",
    "\n",
    "nltk.word_tokenize('A word—any word—is a whole world.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could improve NLTK's tokenizer\n",
    "\n",
    "def tokenize2(string):\n",
    "    \"\"\"\n",
    "    This function will accept a string as an argument,\n",
    "    and return a list of its tokens.\n",
    "    \n",
    "    It will use NLTK's word_tokenize() function as its main process.\n",
    "    But it will also clean up the string *before* passing it to word_tokenize().\n",
    "    \"\"\"\n",
    "    \n",
    "    #@TODO: Finish this function\n",
    "    string = string.replace('—', ' — ')\n",
    "    return nltk.word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " '.',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " '2',\n",
       " 'bananas',\n",
       " '.',\n",
       " '40',\n",
       " '$',\n",
       " 'heuser',\n",
       " '@',\n",
       " 'emaier']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize2(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'word', '—', 'any', 'word', '—', 'is', 'a', 'whole', 'world', '.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize2('A word—any word—is a whole world.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type/Token Ratio\n",
    "\n",
    "A common statistic in corpus linguistics/text mining is the type token ratio (TTR). It is defined simply by\n",
    "\n",
    "    TTR = (# of word types) / (# of word tokens)\n",
    "    \n",
    "Where\n",
    "\n",
    "* \\# of word **types** = the number of *unique* words in a list of words\n",
    "* \\# of word **tokens** = the number of *all* words, including repeated words; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## @TODO: Get the number of tokens for Bobby's paragraph\n",
    "#\n",
    "\n",
    "num_tokens_bobby = len(bobby_list)\n",
    "num_tokens_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Letter',\n",
       " 'came',\n",
       " 'today.',\n",
       " 'Go',\n",
       " 'figure.',\n",
       " 'Never',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'this',\n",
       " 'cousin,',\n",
       " 'but',\n",
       " 'he’s',\n",
       " 'got',\n",
       " 'it',\n",
       " 'all',\n",
       " 'right:',\n",
       " 'name',\n",
       " 'of',\n",
       " 'Bobby’s',\n",
       " 'dad,',\n",
       " 'name',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mom,',\n",
       " 'name',\n",
       " 'of',\n",
       " 'his',\n",
       " 'uncles.',\n",
       " 'Bobby',\n",
       " 'reads',\n",
       " 'it',\n",
       " 'again.',\n",
       " 'Reads',\n",
       " 'it',\n",
       " 'three',\n",
       " 'times.',\n",
       " 'Cousin',\n",
       " 'from',\n",
       " 'Hong',\n",
       " 'Tian',\n",
       " 'in',\n",
       " 'Fujian,',\n",
       " 'same',\n",
       " 'village',\n",
       " 'as',\n",
       " 'his',\n",
       " 'mother’s',\n",
       " 'father.',\n",
       " 'Gotta',\n",
       " 'be',\n",
       " 'a',\n",
       " 'distant',\n",
       " 'cousin.',\n",
       " 'Could',\n",
       " 'be',\n",
       " 'a',\n",
       " 'trick,',\n",
       " 'but',\n",
       " 'could',\n",
       " 'be',\n",
       " 'legit.',\n",
       " 'Besides,',\n",
       " 'how’d',\n",
       " 'he',\n",
       " 'find',\n",
       " 'him?',\n",
       " 'How’d',\n",
       " 'he',\n",
       " 'know',\n",
       " 'Bobby',\n",
       " 'was',\n",
       " 'Chinese',\n",
       " 'from',\n",
       " 'Singapore?',\n",
       " 'Knows',\n",
       " 'everything.',\n",
       " 'Knows',\n",
       " 'Bobby’s',\n",
       " 'Chinese',\n",
       " 'name.',\n",
       " 'Knows',\n",
       " 'about',\n",
       " 'the',\n",
       " 'family',\n",
       " 'bicycle',\n",
       " 'business.',\n",
       " 'Cousin’s',\n",
       " 'in',\n",
       " 'trouble.',\n",
       " 'Musta',\n",
       " 'got',\n",
       " 'smuggled',\n",
       " 'in.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'those',\n",
       " 'boat',\n",
       " 'people.',\n",
       " 'Most',\n",
       " 'never',\n",
       " 'make',\n",
       " 'it.',\n",
       " 'This',\n",
       " 'one',\n",
       " 'might',\n",
       " 'not',\n",
       " 'either.',\n",
       " 'All',\n",
       " 'he’s',\n",
       " 'got',\n",
       " 'is',\n",
       " 'Bobby’s',\n",
       " 'name',\n",
       " 'and',\n",
       " 'address.',\n",
       " 'Now',\n",
       " 'the',\n",
       " 'smugglers',\n",
       " 'want',\n",
       " 'their',\n",
       " 'money.',\n",
       " 'But',\n",
       " 'where’s',\n",
       " 'the',\n",
       " 'cousin?',\n",
       " 'Tijuana.',\n",
       " 'Just',\n",
       " 'turn',\n",
       " 'over',\n",
       " 'the',\n",
       " 'money.',\n",
       " 'Five',\n",
       " 'thou',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'cousin',\n",
       " 'across.',\n",
       " 'If',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'it,',\n",
       " 'five',\n",
       " 'thou',\n",
       " 'to',\n",
       " 'get',\n",
       " 'him',\n",
       " 'free.',\n",
       " 'China',\n",
       " 'to',\n",
       " 'Chinatown.',\n",
       " 'That’s',\n",
       " 'the',\n",
       " 'deal.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bobby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamlet as a list: ['to', 'be', 'or', 'not', 'to', 'be']\n",
      "Hamlet as a SET: {'be', 'to', 'or', 'not'}\n"
     ]
    }
   ],
   "source": [
    "# To get the types, we use the SET data type\n",
    "\n",
    "the_question_as_list = ['to','be','or','not','to','be']\n",
    "the_question_as_set = set(the_question_as_list)\n",
    "\n",
    "print('Hamlet as a list:',the_question_as_list)\n",
    "print('Hamlet as a SET:', the_question_as_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## @TODO: Get the number of types for Bobby's paragraph\n",
    "#\n",
    "\n",
    "bobby_set = set(bobby_list)\n",
    "\n",
    "num_types_bobby = len(bobby_set)\n",
    "num_types_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## @TODO: Print the TTR for Bobby's paragraph\n",
    "#\n",
    "\n",
    "ttr_bobby = num_types_bobby / num_tokens_bobby\n",
    "ttr_bobby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Write a function to calculate the TTR from any *STRING*\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NLTK Goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Rafaela's chapter\n",
    "\n",
    "with open('../corpora/tropic_of_orange/texts/ch01.txt') as file:\n",
    "    rafaela_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Rafaela\n",
    "\n",
    "rafaela_tokens = nltk.word_tokenize(rafaela_str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an NLTK text object\n",
    "\n",
    "rafaela_nltk = nltk.Text(rafaela_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      "ebs , and human hair . an iguana , a crab , and a mouse . and there was the sc\n",
      "ck into the house . the iguana , the crab , and the mouse , for example , were\n",
      "jamming the gears with pieces of the crab , not to mention everything else , a\n",
      "volkswagen van . the story about the crab seemed unlikely . his land was much \n"
     ]
    }
   ],
   "source": [
    "rafaela_nltk.concordance('crab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 18 of 18 matches:\n",
      " . a sweet gooey marmalade from his orange trees , perhaps . but perhaps not .\n",
      "de eight years ago . it was a navel orange tree , maybe the descendent of the \n",
      " detail gabriel liked . bringing an orange tree ( no matter that it was probab\n",
      "ee was a sorry one , and so was the orange . rafaela knew it was an orange tha\n",
      "the orange . rafaela knew it was an orange that should not have been . it was \n",
      "afaela somehow felt this particular orange was special . perhaps it was her de\n",
      "on , she found herself watching the orange , wandering out to the tree every d\n",
      "n and beyond . in the days when the orange was a blossom of soft petals , its \n",
      "ed her . she had passed beneath the orange several times , drawn to its sweet \n",
      "r with pleasure . and when the baby orange appeared , it seemed to grasp that \n",
      "uld be a parent . as expected , the orange did not grow to be very big or seem\n",
      "nd sol walked hand in hand past the orange tree , careful not to disturb the l\n",
      "lstice . she glanced briefly at the orange with some satisfaction and hurried \n",
      "h . rafaela glanced back toward the orange tree and the single orange , sudden\n",
      "ward the orange tree and the single orange , suddenly aware of the only possib\n",
      "eat ball of fire directly above the orange tree . it seemed even to point at t\n",
      "tree , at the strange line , at the orange itself . rafaela ran after sol into\n",
      "usea . she did not realize that the orange had fallen irresistibly from a heig\n"
     ]
    }
   ],
   "source": [
    "rafaela_nltk.concordance('orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO:\n",
    "# - Load one of your texts\n",
    "# - Make a concordance for a word of interest\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-run this line if the figure doesn't show up the first time\n",
    "\n",
    "rafaela_nltk.dispersion_plot(['crab','iguana','orange','tropic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO:\n",
    "# - Load one of your texts\n",
    "# - Make a dispersion plot for a few words of interest\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
