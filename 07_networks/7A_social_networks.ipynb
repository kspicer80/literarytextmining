{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7A) Social networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you'll need to install networkx. In terminal, type:\n",
    "\n",
    "    pip install networkx\n",
    "    pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "Network theory has a famous beginning in a 1736 solution to a mathematical problem called the [Seven Bridges of Königsberg](https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg). The problem: is it possible to cross each of the seven bridges in the city of Königsberg only once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Konigsberg_bridges.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mathematician named [Leonhard Euler](https://en.wikipedia.org/wiki/Leonhard_Euler) proved in 1736 that the problem has no solution (it is not possible to cross each bridge only once). His solution laid the foundations of graph theory, because he discovered that the actual map and geographic space is irrelevant, and all we need to think about are nodes and edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"konigsberg_to_network.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seven bridges become seven \"edges\" between \"nodes\", and we can then demonstrate mathematically that there is no way to move through the network without crossing the same edge twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample graph\n",
    "g_karate = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [source](http://vlado.fmf.uni-lj.si/pub/networks/data/Ucinet/UciData.htm#zachary): \n",
    "\n",
    "<blockquote>\n",
    "BACKGROUND\n",
    "\n",
    "These are data collected from the members of a university karate club by Wayne Zachary. The ZACHE matrix represents the presence or absence of ties among the members of the club; the ZACHC matrix indicates the relative strength of the associations (number of situations in and outside the club in which interactions occurred).\n",
    "\n",
    "Zachary (1977) used these data and an information flow model of network conflict resolution to explain the split-up of this group following disputes among the members.\n",
    "\n",
    "REFERENCE\n",
    "\n",
    "Zachary W. (1977). An information flow model for conflict and fission in small groups. Journal of Anthropological Research, 33, 452-473.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw graph\n",
    "def draw_graph(g):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8));\n",
    "    nx.draw_networkx(g, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(g_karate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph2(g):\n",
    "    from pyvis import network as net\n",
    "    G = net.Network(notebook=True)\n",
    "    G.from_nx(g)\n",
    "    G.show_buttons()\n",
    "    return G.show('graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_graph2(g_karate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nodes\n",
    "g_karate.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the edges\n",
    "g_karate.edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network degree\n",
    "\n",
    "How many other nodes are connected to any given node? This number is called its \"degree.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary from the node --> its \"degree\"\n",
    "degree_dict = dict(g_karate.degree)\n",
    "print(degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick helper function to show a dictionary sorted by its value\n",
    "def sort_dict(d,ascending=False):\n",
    "    # convert the dictionary to a pandas \"series\" (effectively a dataframe column divorced from a dataframe)\n",
    "    series = pd.Series(d)\n",
    "    # return this series, sorted\n",
    "    return series.sort_values(ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which nodes have the most \"ties\"?\n",
    "sort_dict(degree_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same data except normalized by the total number of edges\n",
    "degree_centrality = nx.degree_centrality(g_karate)\n",
    "\n",
    "# Show\n",
    "sort_dict(degree_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweeenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_centrality = nx.betweenness_centrality(g_karate)\n",
    "sort_dict(betweenness_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graph_by_degree(g,min_degree = 2):\n",
    "    # make an empty list which will store all the nodes we want to remove\n",
    "    nodes_to_remove = []\n",
    "    \n",
    "    # get the dictionary of node->degree\n",
    "    degree_dict=dict(g.degree)\n",
    "    \n",
    "    # loop over the nodes\n",
    "    for node in g.nodes():\n",
    "        degree = degree_dict.get(node,0)\n",
    "        if degree < min_degree:\n",
    "            nodes_to_remove.append(node)\n",
    "            \n",
    "    # remove from network\n",
    "    g.remove_nodes_from(nodes_to_remove)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=filter_graph_by_degree(g,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making networks from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to bring a google spreadsheet into a Pandas dataframe\n",
    "def google2df(url):\n",
    "    from io import BytesIO\n",
    "    import requests\n",
    "    r = requests.get(url)\n",
    "    data = r.content\n",
    "    df = pd.read_csv(BytesIO(data))\n",
    "    return df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus metadata for Tropic of Orange\n",
    "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSwmeht7zqyfnQ_Z4LuDTDf4Zr_Ny38jfy0sYmRG4L6oxgH4MynYnhcnFzXq0yMMgPQ8TkuYsKm0Q0Q/pub?gid=0&single=true&output=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = google2df(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this column\n",
    "df['who talks to whom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new graph\n",
    "G_tropic_meta=nx.Graph()\n",
    "\n",
    "# for each entry in this column\n",
    "for entry in df['who talks to whom']:\n",
    "    \n",
    "    # for each comma\n",
    "    for edge_str in entry.split(','):\n",
    "        \n",
    "        # split between characters\n",
    "        chars = edge_str.split('<>')\n",
    "        \n",
    "        # if there are two things in between a \"<>\"\n",
    "        if len(chars)==2:\n",
    "            # get two chars\n",
    "            char1 = chars[0].strip()\n",
    "            char2 = chars[1].strip()\n",
    "            \n",
    "            # add edge between characters\n",
    "            if not G_tropic_meta.has_edge(char1,char2):\n",
    "                G_tropic_meta.add_edge(char1,char2,weight=1)\n",
    "            else:\n",
    "                G_tropic_meta[char1][char2]['weight']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw this graph\n",
    "draw_graph(G_tropic_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making networks from NER\n",
    "\n",
    "Let's make a network of the characters in Harry Potter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy(string):\n",
    "    \"\"\"\n",
    "    Using spacy, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # import spacy\n",
    "        import spacy\n",
    "    except ImportError:\n",
    "        print(\"spacy not installed. Please follow directions above.\")\n",
    "        return\n",
    "\n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "    \n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # create a spacy text object\n",
    "    doc = nlp(string)\n",
    "    \n",
    "    # make an output list\n",
    "    output_list = []\n",
    "\n",
    "    # loop over sentences\n",
    "    sent_num=0\n",
    "    for sent in doc.sents:\n",
    "        sent_num+=1\n",
    "        added_sent_already = False\n",
    "\n",
    "        # loop over sentence's entities\n",
    "        sent_doc = nlp(str(sent))\n",
    "        for ent in sent_doc.ents:\n",
    "            \n",
    "            # make a result dict\n",
    "            result_dict = {}\n",
    "            \n",
    "            # set sentence number\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "            \n",
    "            # store text too\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = sent.text\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "            \n",
    "            # get type\n",
    "            result_dict['type'] = ent.label_\n",
    "            \n",
    "            # get entity\n",
    "            result_dict['entity'] = ent.text\n",
    "            \n",
    "            # get start char\n",
    "            result_dict['start_char'] = ent.start_char\n",
    "            \n",
    "            # get end char\n",
    "            result_dict['end_char'] = ent.end_char\n",
    "            \n",
    "            # add result_dict to output_list\n",
    "            output_list.append(result_dict)\n",
    "            \n",
    "    # return output\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path\n",
    "text_path = '../corpora/harry_potter/texts/Sorcerers Stone.txt'\n",
    "# text_path = '../corpora/tropic_of_orange/texts/ch01.txt'\n",
    "\n",
    "limit_paragraphs = 100\n",
    "#limit_paragraphs = None   # uncomment this to do whole text\n",
    "\n",
    "# open the file\n",
    "with open(text_path) as file:\n",
    "    txt=file.read()\n",
    "\n",
    "# make an empty network\n",
    "G = nx.Graph()\n",
    "    \n",
    "# loop over the paragraphs!\n",
    "paragraphs = txt.split('\\n\\n')\n",
    "\n",
    "# randomize?\n",
    "import random\n",
    "random.shuffle(paragraphs)\n",
    "\n",
    "# limit paragraphs?\n",
    "paragraphs = paragraphs[:limit_paragraphs]\n",
    "\n",
    "para_num=0\n",
    "for para in paragraphs:\n",
    "    para_num+=1\n",
    "    if not para_num%10:\n",
    "        print(para_num,len(paragraphs))\n",
    "    \n",
    "    # get the people for this paragraph\n",
    "    people_in_this_para = []\n",
    "    \n",
    "    # get the NER results for this paragraph\n",
    "    ner_results_ld = ner_spacy(para)\n",
    "    \n",
    "    # for each result\n",
    "    for result_dict in ner_results_ld:\n",
    "        # if it's a person:\n",
    "        if result_dict['type']=='PERSON':\n",
    "            # get the person's name\n",
    "            person=result_dict['entity'].strip()\n",
    "            \n",
    "            # Let's do only first names\n",
    "            if ' ' not in person:\n",
    "                # add the person to the list\n",
    "                people_in_this_para.append(person)\n",
    "    \n",
    "    ## get the unique pairs of persons in this paragraph\n",
    "    # for each person1 in the paragraph\n",
    "    for person1 in people_in_this_para:\n",
    "        # for each person2 in the paragraph\n",
    "        for person2 in people_in_this_para:\n",
    "            # skip if we've repeated this already\n",
    "            if person1>=person2: continue\n",
    "            \n",
    "            \n",
    "            if not G.has_edge(person1,person2):\n",
    "                G.add_edge(person1,person2,weight=1)\n",
    "            else:\n",
    "                G[person1][person2]['weight']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hp=filter_graph_by_degree(G,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(G_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphml (good for Gephi)\n",
    "\n",
    "Try downloading [Gephi](https://gephi.org/) and loading your graphml file for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Graphml (good for Gephi)\n",
    "nx.write_graphml(G, 'data.network.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edgelist (good for Palladio)\n",
    "\n",
    "Try visualizing this in [Palladio](http://hdlab.stanford.edu/palladio-app)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as edgelist\n",
    "\n",
    "def make_edge_table(g):\n",
    "    results=[]\n",
    "    for node1,node2,edge_data in g.edges(data=True):\n",
    "        \n",
    "        edge_data['source'] = node1\n",
    "        edge_data['target'] = node2\n",
    "        \n",
    "        results.append(edge_data)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = make_edge_table(G)\n",
    "edge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df.to_csv('data.network.edges.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For network research team\n",
    "\n",
    "* Generate a network from our metadata:\n",
    "    * Divide up chapters among some people and annotate who speaks to whom [in our metadata spreadsheet](https://docs.google.com/spreadsheets/d/1cRmrwQmq2HuA-cb_mQYGGau40AO9fAROqRUK4EKKoJ4/edit?usp=sharing).\n",
    "    * Generate the network from the metadata\n",
    "    * Who is the most central character? Who is the most betweenness-central character?\n",
    "    * Save the network\n",
    "    * Visualize it in Palladio or Gephi\n",
    "\n",
    "\n",
    "* Generate a network from NER:\n",
    "    * Look at the code above, but adapt it to work for the whole novel\n",
    "    * Visualize it in Gephi or Palladio    \n",
    "\n",
    "\n",
    "* Generate networks by narrator:\n",
    "    * Using either metadata or NER, which narrators' chapters create the most dense social networks? Which the most sparse? \n",
    "    \n",
    "    \n",
    "* (Advanced) Generate a network for other words\n",
    "    * Maybe try this for place names also? Or for people AND placenames together?\n",
    "    * Fruit? Plants? Commodities?\n",
    "\n",
    "\n",
    "* Your own initiatives using networks and *Tropic of Orange*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
