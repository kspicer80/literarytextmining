{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5B) Named Entity Recognition\n",
    "\n",
    "In this second installment of the NLP module, let's look at a more advanced problem: how can we detect places and people in text?\n",
    "\n",
    "Then, we'll practice mapping geographic patterns in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## How can I identify “named entities” (e.g. proper nouns of places and people)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manzanar loves Named Entities in Chapter 35\n",
    "\n",
    "manzanar = \"\"\"Despite everything, every sports event, concert, and whatnot was happening at the same time. L.A.\n",
    "marathoners slouched by the droves across the finish line at the Coliseum. At the Rose Bowl: UCLA versus\n",
    "USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up\n",
    "after throwing its sweaty rider. The Clippers were attempting a comeback in overtime at the Sports Arena.\n",
    "It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold\n",
    "beers and Dodger dogs. Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the\n",
    "last seconds. The Trekkie convention warped into five at the L.A. Convention Center. Bud Girls paraded\n",
    "between boxing matches at the Olympic Auditorium. Plácido Domingo belted Rossini at the Dorothy Chandler\n",
    "under the improbable abstract/minimal/baroque direction of Peter Sellers. At the Shrine, executive\n",
    "producer Richard Sakai accepted an Oscar for the movie version of The Simpsons. The helicopter landed for\n",
    "the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.\n",
    "Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club. Live-laughter\n",
    "sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio\n",
    "in Hollywood and Burbank. Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha ha\n",
    "\n",
    "nasa = \"\"\"Ryan Heuser cannot wait until he graduates from Stanford University.\n",
    "He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some imports\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK\n",
    "\n",
    "We've used NLTK before for word tokenizing, sentence tokenizing, and part of speech tagging. Now let's use it again for named entity recognition using its function `ne_chunk()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ryan Heuser cannot wait until he graduates from Stanford University.',\n",
       " 'He will take up position as Head Engineer of NASA\\'s secret \"Send Literary Critics to Mars\" mission.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 1: Split sentences\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = nltk.sent_tokenize(nasa)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryan', 'Heuser', 'can', 'not', 'wait', 'until', 'he', 'graduates', 'from', 'Stanford', 'University', '.']\n",
      "\n",
      "['He', 'will', 'take', 'up', 'position', 'as', 'Head', 'Engineer', 'of', 'NASA', \"'s\", 'secret', '``', 'Send', 'Literary', 'Critics', 'to', 'Mars', \"''\", 'mission', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Get words from each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    print(sent_words)\n",
    "    \n",
    "    # empty line\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ryan', 'NNP'), ('Heuser', 'NNP'), ('can', 'MD'), ('not', 'RB'), ('wait', 'VB'), ('until', 'IN'), ('he', 'PRP'), ('graduates', 'VBZ'), ('from', 'IN'), ('Stanford', 'NNP'), ('University', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('He', 'PRP'), ('will', 'MD'), ('take', 'VB'), ('up', 'RP'), ('position', 'NN'), ('as', 'IN'), ('Head', 'NNP'), ('Engineer', 'NNP'), ('of', 'IN'), ('NASA', 'NNP'), (\"'s\", 'POS'), ('secret', 'JJ'), ('``', '``'), ('Send', 'NNP'), ('Literary', 'NNP'), ('Critics', 'NNPS'), ('to', 'TO'), ('Mars', 'NNP'), (\"''\", \"''\"), ('mission', 'NN'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Get POS for each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    print(sent_pos)\n",
    "    \n",
    "    # empty line\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Ryan/NNP)\n",
      "  (PERSON Heuser/NNP)\n",
      "  can/MD\n",
      "  not/RB\n",
      "  wait/VB\n",
      "  until/IN\n",
      "  he/PRP\n",
      "  graduates/VBZ\n",
      "  from/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ./.)\n",
      "\n",
      "(S\n",
      "  He/PRP\n",
      "  will/MD\n",
      "  take/VB\n",
      "  up/RP\n",
      "  position/NN\n",
      "  as/IN\n",
      "  (PERSON Head/NNP Engineer/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  's/POS\n",
      "  secret/JJ\n",
      "  ``/``\n",
      "  Send/NNP\n",
      "  Literary/NNP\n",
      "  Critics/NNPS\n",
      "  to/TO\n",
      "  (PERSON Mars/NNP)\n",
      "  ''/''\n",
      "  mission/NN\n",
      "  ./.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 4: Get named entity 'chunks' for each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "    print(chunks)\n",
    "    \n",
    "    # empty line\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON\n",
      "[('Ryan', 'NNP')]\n",
      "\n",
      "PERSON\n",
      "[('Heuser', 'NNP')]\n",
      "\n",
      "ORGANIZATION\n",
      "[('Stanford', 'NNP'), ('University', 'NNP')]\n",
      "\n",
      "PERSON\n",
      "[('Head', 'NNP'), ('Engineer', 'NNP')]\n",
      "\n",
      "ORGANIZATION\n",
      "[('NASA', 'NNP')]\n",
      "\n",
      "PERSON\n",
      "[('Mars', 'NNP')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 5: Get information about the named entity chunks\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "\n",
    "    # loop over each one\n",
    "    for chunk in chunks:\n",
    "        #print(chunk)\n",
    "        #continue\n",
    "\n",
    "        # if the chunk has a 'label' attribute (is a named entity)\n",
    "        if hasattr(chunk,'label'):\n",
    "\n",
    "            # get the label\n",
    "            label = chunk.label()\n",
    "\n",
    "            # print\n",
    "            print(label)\n",
    "            print(list(chunk))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanford University'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [('Stanford', 'NNP'), ('University', 'NNP')]\n",
    "\n",
    "l = []\n",
    "for word,tag in x:\n",
    "    l.append(word)\n",
    "    \n",
    "\" \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello\\nthis\\nis\\nRyan'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ['Hello','this','is','Ryan']\n",
    "\n",
    "''.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON : Ryan\n",
      "PERSON : Heuser\n",
      "ORGANIZATION : Stanford University\n",
      "PERSON : Head Engineer\n",
      "ORGANIZATION : NASA\n",
      "PERSON : Mars\n"
     ]
    }
   ],
   "source": [
    "### Step 5: Get information about the named entity chunks\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "\n",
    "    # loop over each one\n",
    "    for chunk in chunks:\n",
    "\n",
    "        # if the chunk has a 'label' attribute (is a named entity)\n",
    "        if hasattr(chunk,'label'):\n",
    "\n",
    "            # get the label\n",
    "            label = chunk.label()\n",
    "\n",
    "            # get the words\n",
    "            chunk_words = []\n",
    "            for word,tag in chunk:\n",
    "                chunk_words.append(word)\n",
    "\n",
    "            # make a string version\n",
    "            chunk_words_str = ' '.join(chunk_words)\n",
    "            \n",
    "            print(label,':',chunk_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's make a function for this\n",
    "def ner_nltk(string):\n",
    "    \"\"\"\n",
    "    Using NLTK, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ')\n",
    "    \n",
    "    # sentence tokenize the string\n",
    "    sentences = nltk.sent_tokenize(string)\n",
    "    \n",
    "    # set empty list for output\n",
    "    output_list = []\n",
    "    \n",
    "    # loop over each sentence\n",
    "    sent_num = 0\n",
    "    for sent in sentences:\n",
    "        # add 1 to sent num\n",
    "        sent_num+=1\n",
    "        \n",
    "        # default this to False (see why below)\n",
    "        added_sent_already = False\n",
    "        \n",
    "        # we need to get the words\n",
    "        sent_words = nltk.word_tokenize(sent)\n",
    "        \n",
    "        # parts of speech\n",
    "        sent_pos = nltk.pos_tag(sent_words)\n",
    "        \n",
    "        # then \"chunk\"\n",
    "        chunks = nltk.ne_chunk(sent_pos)\n",
    "        \n",
    "        # loop over chunks...\n",
    "        for chunk in chunks:\n",
    "            # if the chunk has a 'label' attribute (is a named entity)\n",
    "            if hasattr(chunk,'label'):\n",
    "                \n",
    "                # get the label\n",
    "                label = chunk.label()\n",
    "                \n",
    "                # get the words in the chunk\n",
    "                chunk_words = []\n",
    "                for word,pos in chunk:\n",
    "                    chunk_words.append(word)\n",
    "                \n",
    "                # make a string version\n",
    "                chunk_words_str = ' '.join(chunk_words)\n",
    "                \n",
    "                # make a result dictionary\n",
    "                result_dict = {}\n",
    "                \n",
    "                # add NER info\n",
    "                result_dict['type'] = label\n",
    "                result_dict['entity'] = chunk_words_str\n",
    "                \n",
    "                ### optional: add sent info\n",
    "                result_dict['_sent_num'] = sent_num\n",
    "                # add a string of the sentence, but only once per sentence\n",
    "                if not added_sent_already:\n",
    "                    result_dict['_sent'] = sent\n",
    "                    added_sent_already = True\n",
    "                else:\n",
    "                    result_dict['_sent'] = ''\n",
    "                ###\n",
    "                \n",
    "                # add result dictionary to output list\n",
    "                output_list.append(result_dict)\n",
    "    \n",
    "    # return list of dictionaries\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ner_nltk in module __main__:\n",
      "\n",
      "ner_nltk(string)\n",
      "    Using NLTK, this function takes any string, identifies the named entities in it,\n",
      "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
      "    where each dictionary looks like this:\n",
      "    \n",
      "    {\n",
      "        'type': 'PERSON',\n",
      "        'entity': 'Ryan',\n",
      "        '_sent_num': 1,\n",
      "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ner_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'PERSON',\n",
       "  'entity': 'Ryan',\n",
       "  '_sent_num': 1,\n",
       "  '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'},\n",
       " {'type': 'PERSON', 'entity': 'Heuser', '_sent_num': 1, '_sent': ''},\n",
       " {'type': 'ORGANIZATION',\n",
       "  'entity': 'Stanford University',\n",
       "  '_sent_num': 1,\n",
       "  '_sent': ''},\n",
       " {'type': 'PERSON',\n",
       "  'entity': 'Head Engineer',\n",
       "  '_sent_num': 2,\n",
       "  '_sent': 'He will take up position as Head Engineer of NASA\\'s secret \"Send Literary Critics to Mars\" mission.'},\n",
       " {'type': 'ORGANIZATION', 'entity': 'NASA', '_sent_num': 2, '_sent': ''},\n",
       " {'type': 'PERSON', 'entity': 'Mars', '_sent_num': 2, '_sent': ''}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_ner_ld = ner_nltk(nasa)\n",
    "nltk_ner_ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sent</th>\n",
       "      <th>_sent_num</th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan Heuser cannot wait until he graduates from Stanford University.</td>\n",
       "      <td>1</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Heuser</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.</td>\n",
       "      <td>2</td>\n",
       "      <td>Head Engineer</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>NASA</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Mars</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 _sent  \\\n",
       "0  Ryan Heuser cannot wait until he graduates from Stanford University.                                  \n",
       "1                                                                                                        \n",
       "2                                                                                                        \n",
       "3  He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.   \n",
       "4                                                                                                        \n",
       "5                                                                                                        \n",
       "\n",
       "   _sent_num               entity          type  \n",
       "0  1          Ryan                 PERSON        \n",
       "1  1          Heuser               PERSON        \n",
       "2  1          Stanford University  ORGANIZATION  \n",
       "3  2          Head Engineer        PERSON        \n",
       "4  2          NASA                 ORGANIZATION  \n",
       "5  2          Mars                 PERSON        "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_ner_df = pd.DataFrame(nltk_ner_ld)\n",
    "nltk_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'GPE',\n",
       " 'entity': 'Coliseum',\n",
       " '_sent_num': 2,\n",
       " '_sent': 'L.A. marathoners slouched by the droves across the finish line at the Coliseum.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_ner_ld = ner_nltk(manzanar)\n",
    "nltk_ner_ld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sent</th>\n",
       "      <th>_sent_num</th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L.A. marathoners slouched by the droves across the finish line at the Coliseum.</td>\n",
       "      <td>2</td>\n",
       "      <td>Coliseum</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the Rose Bowl: UCLA versus USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up after throwing its sweaty rider.</td>\n",
       "      <td>3</td>\n",
       "      <td>Rose</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>USC</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Bruin</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Trojan</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Clippers were attempting a comeback in overtime at the Sports Arena.</td>\n",
       "      <td>4</td>\n",
       "      <td>Clippers</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>Sports Arena</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold beers and Dodger dogs.</td>\n",
       "      <td>5</td>\n",
       "      <td>Nomo</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Chavez Ravine</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Dodger</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the last seconds.</td>\n",
       "      <td>6</td>\n",
       "      <td>Scottie</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>Pippen</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>Shaq</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Trekkie convention warped into five at the L.A. Convention Center.</td>\n",
       "      <td>7</td>\n",
       "      <td>Trekkie</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>L.A. Convention Center</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bud Girls paraded between boxing matches at the Olympic Auditorium.</td>\n",
       "      <td>8</td>\n",
       "      <td>Bud</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Girls</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Plácido Domingo belted Rossini at the Dorothy Chandler under the improbable abstract/minimal/baroque direction of Peter Sellers.</td>\n",
       "      <td>9</td>\n",
       "      <td>Plácido</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Domingo</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Rossini</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Dorothy</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Peter Sellers</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>At the Shrine, executive producer Richard Sakai accepted an Oscar for the movie version of The Simpsons.</td>\n",
       "      <td>10</td>\n",
       "      <td>Shrine</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>Richard Sakai</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>Simpsons</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The helicopter landed for the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.</td>\n",
       "      <td>11</td>\n",
       "      <td>Miss Saigon</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>Ahmanson</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>Shubert</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club.</td>\n",
       "      <td>12</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>Asian</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>Bicycle Club</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Live-laughter sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio in Hollywood and Burbank.</td>\n",
       "      <td>13</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>Burbank</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.</td>\n",
       "      <td>14</td>\n",
       "      <td>Julio Iglesias</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>Universal Amphitheater</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                     _sent  \\\n",
       "0   L.A. marathoners slouched by the droves across the finish line at the Coliseum.                                                                                          \n",
       "1   At the Rose Bowl: UCLA versus USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up after throwing its sweaty rider.   \n",
       "2                                                                                                                                                                            \n",
       "3                                                                                                                                                                            \n",
       "4                                                                                                                                                                            \n",
       "5   The Clippers were attempting a comeback in overtime at the Sports Arena.                                                                                                 \n",
       "6                                                                                                                                                                            \n",
       "7   It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold beers and Dodger dogs.                                         \n",
       "8                                                                                                                                                                            \n",
       "9                                                                                                                                                                            \n",
       "10  Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the last seconds.                                                                        \n",
       "11                                                                                                                                                                           \n",
       "12                                                                                                                                                                           \n",
       "13  The Trekkie convention warped into five at the L.A. Convention Center.                                                                                                   \n",
       "14                                                                                                                                                                           \n",
       "15  Bud Girls paraded between boxing matches at the Olympic Auditorium.                                                                                                      \n",
       "16                                                                                                                                                                           \n",
       "17  Plácido Domingo belted Rossini at the Dorothy Chandler under the improbable abstract/minimal/baroque direction of Peter Sellers.                                         \n",
       "18                                                                                                                                                                           \n",
       "19                                                                                                                                                                           \n",
       "20                                                                                                                                                                           \n",
       "21                                                                                                                                                                           \n",
       "22  At the Shrine, executive producer Richard Sakai accepted an Oscar for the movie version of The Simpsons.                                                                 \n",
       "23                                                                                                                                                                           \n",
       "24                                                                                                                                                                           \n",
       "25  The helicopter landed for the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.                                         \n",
       "26                                                                                                                                                                           \n",
       "27                                                                                                                                                                           \n",
       "28                                                                                                                                                                           \n",
       "29  Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club.                                                                             \n",
       "30                                                                                                                                                                           \n",
       "31                                                                                                                                                                           \n",
       "32  Live-laughter sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio in Hollywood and Burbank.                         \n",
       "33                                                                                                                                                                           \n",
       "34  Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.                                                                                         \n",
       "35                                                                                                                                                                           \n",
       "\n",
       "    _sent_num                  entity          type  \n",
       "0   2          Coliseum                GPE           \n",
       "1   3          Rose                    ORGANIZATION  \n",
       "2   3          USC                     ORGANIZATION  \n",
       "3   3          Bruin                   GPE           \n",
       "4   3          Trojan                  GPE           \n",
       "5   4          Clippers                ORGANIZATION  \n",
       "6   4          Sports Arena            ORGANIZATION  \n",
       "7   5          Nomo                    ORGANIZATION  \n",
       "8   5          Chavez Ravine           ORGANIZATION  \n",
       "9   5          Dodger                  GPE           \n",
       "10  6          Scottie                 PERSON        \n",
       "11  6          Pippen                  PERSON        \n",
       "12  6          Shaq                    PERSON        \n",
       "13  7          Trekkie                 ORGANIZATION  \n",
       "14  7          L.A. Convention Center  ORGANIZATION  \n",
       "15  8          Bud                     PERSON        \n",
       "16  8          Girls                   PERSON        \n",
       "17  9          Plácido                 PERSON        \n",
       "18  9          Domingo                 PERSON        \n",
       "19  9          Rossini                 PERSON        \n",
       "20  9          Dorothy                 ORGANIZATION  \n",
       "21  9          Peter Sellers           PERSON        \n",
       "22  10         Shrine                  GPE           \n",
       "23  10         Richard Sakai           PERSON        \n",
       "24  10         Simpsons                ORGANIZATION  \n",
       "25  11         Miss Saigon             ORGANIZATION  \n",
       "26  11         Ahmanson                ORGANIZATION  \n",
       "27  11         Beauty                  PERSON        \n",
       "28  11         Shubert                 ORGANIZATION  \n",
       "29  12         Chinese                 GPE           \n",
       "30  12         Asian                   GPE           \n",
       "31  12         Bicycle Club            ORGANIZATION  \n",
       "32  13         Hollywood               GPE           \n",
       "33  13         Burbank                 GPE           \n",
       "34  14         Julio Iglesias          PERSON        \n",
       "35  14         Universal Amphitheater  ORGANIZATION  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_ner_df = pd.DataFrame(nltk_ner_ld)\n",
    "nltk_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this data!\n",
    "nltk_ner_df.to_excel('data.ner_nltk.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Polyglot (for non-English, non-French, non-German text)\n",
    "\n",
    "[Polyglot](https://polyglot.readthedocs.io/) is a really cool package, built on top of TextBlob, which supports up to 140 different languages depending on the NLP task. This increase in linguistic range comes at a cost of accuracy, however. The tools was trained using Wikipedia as a Rosetta Stone, calibrating languages' models against each other by using the \"same\" articles in those languages. The other costs of using Polyglot: the documentation isn't that great, and it doesn't seem to be actively updated.\n",
    "\n",
    "*Installation is also kind of a pain in the neck.* I recommend installing this only if you are planning to work with non-English, non-French, non-German text. To do so, paste the following into Terminal:\n",
    "\n",
    "    conda install -c conda-forge pyicu\n",
    "    pip install pycld2\n",
    "    pip install morfessor\n",
    "    pip install polyglot\n",
    "    polyglot download LANG:en   # for english\n",
    "    polyglot download LANG:es   # for spanish (optional)\n",
    "    polyglot download LANG:xx   # where xx is the two-letter language code\n",
    "   \n",
    "See [the website](https://polyglot.readthedocs.io/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_polyglot(string):\n",
    "    \"\"\"\n",
    "    Using polyglot, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"    \n",
    "    \n",
    "    # let's try this...\n",
    "    try:\n",
    "        # to use polyglot, import its \"Text\" object:\n",
    "        from polyglot.text import Text\n",
    "    except ImportError:\n",
    "        print('Polyglot not installed! To do so, follow the instructions above.')\n",
    "        return\n",
    "    # from here on we can assume that polyglot is imported\n",
    "    \n",
    "    # wrap that Text object around any string\n",
    "    pg_text = Text(string)\n",
    "\n",
    "    # make an output list\n",
    "    output_list = []\n",
    "    \n",
    "    # get the entities\n",
    "    entities = pg_text.entities\n",
    "    # loop over sentences\n",
    "    sent_num = 0\n",
    "    for sent in pg_text.sentences:\n",
    "        sent_num+=1\n",
    "\n",
    "        # loop over the entities\n",
    "        added_sent_already = False\n",
    "        for ent in sent.entities:\n",
    "            # get the type\n",
    "            ent_type = ent.tag\n",
    "\n",
    "            # get the words\n",
    "            ent_words = list(ent)\n",
    "\n",
    "            # bogus if the first letter of the first word is not alphabetic (not punctuation)\n",
    "            if not ent_words[0][0].isalpha(): continue\n",
    "\n",
    "            # make a string version\n",
    "            ent_words_str = ' '.join(ent_words)\n",
    "\n",
    "            # make a results dict\n",
    "            result_dict = {}\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = str(sent)\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "            \n",
    "            result_dict['type']=ent_type\n",
    "            result_dict['entity']=ent_words_str\n",
    "\n",
    "            # add to output\n",
    "            output_list.append(result_dict)\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(ner_polyglot(nasa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Manzanar's paragraph\n",
    "#pd.DataFrame(ner_polyglot(manzanar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Spacy\n",
    "\n",
    "[Spacy](http://spacy.io) is industrial-strength NLP. It's the fastest, most powerful, and most accurate. It can also work on [several languages besides English](https://spacy.io/models). But it's also kinda ugly and confusing to use. I recommend using this only if you are working on hundreds of texts and feel extremely comfortable with all the things we've been doing so far.\n",
    "\n",
    "To install:\n",
    "\n",
    "    pip install spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "Here's an NER implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy(string):\n",
    "    \"\"\"\n",
    "    Using spacy, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # import spacy\n",
    "        import spacy\n",
    "    except ImportError:\n",
    "        print(\"spacy not installed. Please follow directions above.\")\n",
    "        return\n",
    "\n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "    \n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # create a spacy text object\n",
    "    doc = nlp(string)\n",
    "    \n",
    "    # make an output list\n",
    "    output_list = []\n",
    "\n",
    "    # loop over sentences\n",
    "    sent_num=0\n",
    "    for sent in doc.sents:\n",
    "        sent_num+=1\n",
    "        added_sent_already = False\n",
    "\n",
    "        # loop over sentence's entities\n",
    "        sent_doc = nlp(str(sent))\n",
    "        for ent in sent_doc.ents:\n",
    "            \n",
    "            # make a result dict\n",
    "            result_dict = {}\n",
    "            \n",
    "            # set sentence number\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "            \n",
    "            # store text too\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = sent.text\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "            \n",
    "            # get type\n",
    "            result_dict['type'] = ent.label_\n",
    "            \n",
    "            # get entity\n",
    "            result_dict['entity'] = ent.text\n",
    "            \n",
    "            # get start char\n",
    "            result_dict['start_char'] = ent.start_char\n",
    "            \n",
    "            # get end char\n",
    "            result_dict['end_char'] = ent.end_char\n",
    "            \n",
    "            # add result_dict to output_list\n",
    "            output_list.append(result_dict)\n",
    "            \n",
    "    # return output\n",
    "    return output_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sent</th>\n",
       "      <th>_sent_num</th>\n",
       "      <th>end_char</th>\n",
       "      <th>entity</th>\n",
       "      <th>start_char</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan Heuser cannot wait until he graduates from Stanford University.</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Ryan Heuser</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>48</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>33</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>NASA</td>\n",
       "      <td>45</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>Send Literary Critics to Mars</td>\n",
       "      <td>60</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 _sent  \\\n",
       "0  Ryan Heuser cannot wait until he graduates from Stanford University.                                  \n",
       "1                                                                                                        \n",
       "2  He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.   \n",
       "3                                                                                                        \n",
       "4                                                                                                        \n",
       "\n",
       "   _sent_num  end_char                         entity  start_char         type  \n",
       "0  1          11        Ryan Heuser                    0           PERSON       \n",
       "1  1          67        Stanford University            48          ORG          \n",
       "2  2          41        Engineer                       33          PERSON       \n",
       "3  2          49        NASA                           45          ORG          \n",
       "4  2          89        Send Literary Critics to Mars  60          WORK_OF_ART  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ner_spacy(nasa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_sent_num': 2,\n",
       " '_sent': 'L.A. marathoners slouched by the droves across the finish line at the Coliseum.',\n",
       " 'type': 'GPE',\n",
       " 'entity': 'Coliseum',\n",
       " 'start_char': 70,\n",
       " 'end_char': 78}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_ner_ld = ner_spacy(manzanar)\n",
    "spacy_ner_ld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sent</th>\n",
       "      <th>_sent_num</th>\n",
       "      <th>end_char</th>\n",
       "      <th>entity</th>\n",
       "      <th>start_char</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L.A. marathoners slouched by the droves across the finish line at the Coliseum.</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>Coliseum</td>\n",
       "      <td>70</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the Rose Bowl: UCLA versus USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up after throwing its sweaty rider.</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>the Rose Bowl</td>\n",
       "      <td>3</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>USC</td>\n",
       "      <td>30</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Bruin</td>\n",
       "      <td>39</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>Trojan</td>\n",
       "      <td>109</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Clippers were attempting a comeback in overtime at the Sports Arena.</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>Clippers</td>\n",
       "      <td>4</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>the Sports Arena</td>\n",
       "      <td>55</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold beers and Dodger dogs.</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>Nomo</td>\n",
       "      <td>49</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>Chavez Ravine</td>\n",
       "      <td>62</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>Dodger</td>\n",
       "      <td>116</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the last seconds.</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>Scottie Pippen</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>Shaq</td>\n",
       "      <td>22</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Trekkie convention warped into five at the L.A. Convention Center.</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>Trekkie</td>\n",
       "      <td>4</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>five</td>\n",
       "      <td>35</td>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>the L.A. Convention Center</td>\n",
       "      <td>43</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bud Girls paraded between boxing matches at the Olympic Auditorium.</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>Bud Girls</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>the Olympic Auditorium</td>\n",
       "      <td>44</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Plácido Domingo belted Rossini at the Dorothy Chandler under the improbable abstract/minimal/baroque direction of Peter Sellers.</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>Plácido Domingo</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>Rossini</td>\n",
       "      <td>23</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>Dorothy Chandler</td>\n",
       "      <td>38</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>127</td>\n",
       "      <td>Peter Sellers</td>\n",
       "      <td>114</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>At the Shrine, executive producer Richard Sakai accepted an Oscar for the movie version of The Simpsons.</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>Shrine</td>\n",
       "      <td>7</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>Richard Sakai</td>\n",
       "      <td>34</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>65</td>\n",
       "      <td>Oscar</td>\n",
       "      <td>60</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>91</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The helicopter landed for the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.</td>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>88</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>127</td>\n",
       "      <td>Shubert</td>\n",
       "      <td>120</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club.</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>Asian</td>\n",
       "      <td>61</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>91</td>\n",
       "      <td>the Bicycle Club</td>\n",
       "      <td>75</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Live-laughter sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio in Hollywood and Burbank.</td>\n",
       "      <td>13</td>\n",
       "      <td>131</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>122</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>143</td>\n",
       "      <td>Burbank</td>\n",
       "      <td>136</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>0</td>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>Julio Iglesias</td>\n",
       "      <td>35</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                     _sent  \\\n",
       "0   L.A. marathoners slouched by the droves across the finish line at the Coliseum.                                                                                          \n",
       "1   At the Rose Bowl: UCLA versus USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up after throwing its sweaty rider.   \n",
       "2                                                                                                                                                                            \n",
       "3                                                                                                                                                                            \n",
       "4                                                                                                                                                                            \n",
       "5   The Clippers were attempting a comeback in overtime at the Sports Arena.                                                                                                 \n",
       "6                                                                                                                                                                            \n",
       "7   It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold beers and Dodger dogs.                                         \n",
       "8                                                                                                                                                                            \n",
       "9                                                                                                                                                                            \n",
       "10  Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the last seconds.                                                                        \n",
       "11                                                                                                                                                                           \n",
       "12  The Trekkie convention warped into five at the L.A. Convention Center.                                                                                                   \n",
       "13                                                                                                                                                                           \n",
       "14                                                                                                                                                                           \n",
       "15  Bud Girls paraded between boxing matches at the Olympic Auditorium.                                                                                                      \n",
       "16                                                                                                                                                                           \n",
       "17  Plácido Domingo belted Rossini at the Dorothy Chandler under the improbable abstract/minimal/baroque direction of Peter Sellers.                                         \n",
       "18                                                                                                                                                                           \n",
       "19                                                                                                                                                                           \n",
       "20                                                                                                                                                                           \n",
       "21  At the Shrine, executive producer Richard Sakai accepted an Oscar for the movie version of The Simpsons.                                                                 \n",
       "22                                                                                                                                                                           \n",
       "23                                                                                                                                                                           \n",
       "24                                                                                                                                                                           \n",
       "25  The helicopter landed for the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.                                         \n",
       "26                                                                                                                                                                           \n",
       "27  Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club.                                                                             \n",
       "28                                                                                                                                                                           \n",
       "29                                                                                                                                                                           \n",
       "30  Live-laughter sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio in Hollywood and Burbank.                         \n",
       "31                                                                                                                                                                           \n",
       "32  Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.                                                                                         \n",
       "33                                                                                                                                                                           \n",
       "\n",
       "    _sent_num  end_char                      entity  start_char         type  \n",
       "0   2          78        Coliseum                    70          GPE          \n",
       "1   3          16        the Rose Bowl               3           FAC          \n",
       "2   3          33        USC                         30          ORG          \n",
       "3   3          44        Bruin                       39          NORP         \n",
       "4   3          115       Trojan                      109         GPE          \n",
       "5   4          12        Clippers                    4           ORG          \n",
       "6   4          71        the Sports Arena            55          FAC          \n",
       "7   5          53        Nomo                        49          ORG          \n",
       "8   5          75        Chavez Ravine               62          PERSON       \n",
       "9   5          122       Dodger                      116         NORP         \n",
       "10  6          14        Scottie Pippen              0           PERSON       \n",
       "11  6          26        Shaq                        22          PERSON       \n",
       "12  7          11        Trekkie                     4           ORG          \n",
       "13  7          39        five                        35          CARDINAL     \n",
       "14  7          69        the L.A. Convention Center  43          FAC          \n",
       "15  8          9         Bud Girls                   0           PERSON       \n",
       "16  8          66        the Olympic Auditorium      44          FAC          \n",
       "17  9          15        Plácido Domingo             0           PERSON       \n",
       "18  9          30        Rossini                     23          PERSON       \n",
       "19  9          54        Dorothy Chandler            38          PERSON       \n",
       "20  9          127       Peter Sellers               114         PERSON       \n",
       "21  10         13        Shrine                      7           FAC          \n",
       "22  10         47        Richard Sakai               34          PERSON       \n",
       "23  10         65        Oscar                       60          PERSON       \n",
       "24  10         103       The Simpsons                91          WORK_OF_ART  \n",
       "25  11         94        Beauty                      88          ORG          \n",
       "26  11         127       Shubert                     120         PERSON       \n",
       "27  12         7         Chinese                     0           NORP         \n",
       "28  12         66        Asian                       61          NORP         \n",
       "29  12         91        the Bicycle Club            75          ORG          \n",
       "30  13         131       Hollywood                   122         GPE          \n",
       "31  13         143       Burbank                     136         GPE          \n",
       "32  14         9         Thousands                   0           CARDINAL     \n",
       "33  14         49        Julio Iglesias              35          ORG          "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_ner_df = pd.DataFrame(spacy_ner_ld)\n",
    "spacy_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results!\n",
    "spacy_ner_df.to_excel('data.ner_spacy.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting counts from the data\n",
    "\n",
    "Use `value_counts()` to count the values in any column of a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORGANIZATION    15\n",
       "PERSON          12\n",
       "GPE             9 \n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the counts for the column 'type'\n",
    "val_counts_type = nltk_ner_df['type'].value_counts()\n",
    "val_counts_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Universal Amphitheater    1\n",
       "Plácido                   1\n",
       "Dodger                    1\n",
       "Girls                     1\n",
       "USC                       1\n",
       "Chinese                   1\n",
       "Shaq                      1\n",
       "L.A. Convention Center    1\n",
       "Scottie                   1\n",
       "Simpsons                  1\n",
       "Peter Sellers             1\n",
       "Rossini                   1\n",
       "Richard Sakai             1\n",
       "Nomo                      1\n",
       "Trekkie                   1\n",
       "Coliseum                  1\n",
       "Bud                       1\n",
       "Ahmanson                  1\n",
       "Sports Arena              1\n",
       "Hollywood                 1\n",
       "Dorothy                   1\n",
       "Miss Saigon               1\n",
       "Julio Iglesias            1\n",
       "Shrine                    1\n",
       "Asian                     1\n",
       "Burbank                   1\n",
       "Chavez Ravine             1\n",
       "Rose                      1\n",
       "Beauty                    1\n",
       "Bruin                     1\n",
       "Domingo                   1\n",
       "Shubert                   1\n",
       "Trojan                    1\n",
       "Pippen                    1\n",
       "Clippers                  1\n",
       "Bicycle Club              1\n",
       "Name: entity, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the counts for the column 'entity'\n",
    "val_counts_entity = nltk_ner_df['entity'].value_counts()\n",
    "val_counts_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /Users/ryan/anaconda3/bin/spacy: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save any of these val_counts as an excel file itself\n",
    "val_counts_entity.to_excel('data.ner_nltk_entity_counts.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ORGANIZATION': 15, 'PERSON': 12, 'GPE': 9}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also convert these to dictionaries\n",
    "dict(val_counts_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting in multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity                  type        \n",
       "Ahmanson                ORGANIZATION    1\n",
       "Asian                   GPE             1\n",
       "Beauty                  PERSON          1\n",
       "Bicycle Club            ORGANIZATION    1\n",
       "Bruin                   GPE             1\n",
       "Bud                     PERSON          1\n",
       "Burbank                 GPE             1\n",
       "Chavez Ravine           ORGANIZATION    1\n",
       "Chinese                 GPE             1\n",
       "Clippers                ORGANIZATION    1\n",
       "Coliseum                GPE             1\n",
       "Dodger                  GPE             1\n",
       "Domingo                 PERSON          1\n",
       "Dorothy                 ORGANIZATION    1\n",
       "Girls                   PERSON          1\n",
       "Hollywood               GPE             1\n",
       "Julio Iglesias          PERSON          1\n",
       "L.A. Convention Center  ORGANIZATION    1\n",
       "Miss Saigon             ORGANIZATION    1\n",
       "Nomo                    ORGANIZATION    1\n",
       "Peter Sellers           PERSON          1\n",
       "Pippen                  PERSON          1\n",
       "Plácido                 PERSON          1\n",
       "Richard Sakai           PERSON          1\n",
       "Rose                    ORGANIZATION    1\n",
       "Rossini                 PERSON          1\n",
       "Scottie                 PERSON          1\n",
       "Shaq                    PERSON          1\n",
       "Shrine                  GPE             1\n",
       "Shubert                 ORGANIZATION    1\n",
       "Simpsons                ORGANIZATION    1\n",
       "Sports Arena            ORGANIZATION    1\n",
       "Trekkie                 ORGANIZATION    1\n",
       "Trojan                  GPE             1\n",
       "USC                     ORGANIZATION    1\n",
       "Universal Amphitheater  ORGANIZATION    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To count a combination of multiple columns, use .groupby() followed by .size()\n",
    "\n",
    "val_counts_entity_type = nltk_ner_df.groupby(['entity','type']).size()\n",
    "val_counts_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save this to an excel file too\n",
    "val_counts_entity_type.to_excel('data.ner_nltk_entity_type_counts.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Ahmanson', 'ORGANIZATION'): 1,\n",
       " ('Asian', 'GPE'): 1,\n",
       " ('Beauty', 'PERSON'): 1,\n",
       " ('Bicycle Club', 'ORGANIZATION'): 1,\n",
       " ('Bruin', 'GPE'): 1,\n",
       " ('Bud', 'PERSON'): 1,\n",
       " ('Burbank', 'GPE'): 1,\n",
       " ('Chavez Ravine', 'ORGANIZATION'): 1,\n",
       " ('Chinese', 'GPE'): 1,\n",
       " ('Clippers', 'ORGANIZATION'): 1,\n",
       " ('Coliseum', 'GPE'): 1,\n",
       " ('Dodger', 'GPE'): 1,\n",
       " ('Domingo', 'PERSON'): 1,\n",
       " ('Dorothy', 'ORGANIZATION'): 1,\n",
       " ('Girls', 'PERSON'): 1,\n",
       " ('Hollywood', 'GPE'): 1,\n",
       " ('Julio Iglesias', 'PERSON'): 1,\n",
       " ('L.A. Convention Center', 'ORGANIZATION'): 1,\n",
       " ('Miss Saigon', 'ORGANIZATION'): 1,\n",
       " ('Nomo', 'ORGANIZATION'): 1,\n",
       " ('Peter Sellers', 'PERSON'): 1,\n",
       " ('Pippen', 'PERSON'): 1,\n",
       " ('Plácido', 'PERSON'): 1,\n",
       " ('Richard Sakai', 'PERSON'): 1,\n",
       " ('Rose', 'ORGANIZATION'): 1,\n",
       " ('Rossini', 'PERSON'): 1,\n",
       " ('Scottie', 'PERSON'): 1,\n",
       " ('Shaq', 'PERSON'): 1,\n",
       " ('Shrine', 'GPE'): 1,\n",
       " ('Shubert', 'ORGANIZATION'): 1,\n",
       " ('Simpsons', 'ORGANIZATION'): 1,\n",
       " ('Sports Arena', 'ORGANIZATION'): 1,\n",
       " ('Trekkie', 'ORGANIZATION'): 1,\n",
       " ('Trojan', 'GPE'): 1,\n",
       " ('USC', 'ORGANIZATION'): 1,\n",
       " ('Universal Amphitheater', 'ORGANIZATION'): 1}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can convert this to a dictionary too\n",
    "dict(val_counts_entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "**@TODO: Make a map of all mentioned places in *Tropic of Orange***\n",
    "\n",
    "Follow the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Get the named entities for the entire Tropic of Orange text\n",
    "#\n",
    "\n",
    "# Load the dataframe for Tropic of Orange\n",
    "df_tropic = pd.read_excel('../corpora/tropic_of_orange/metadata.xls')\n",
    "\n",
    "# make an empty list for all results in the book\n",
    "all_results = []\n",
    "\n",
    "# set a variable to the text folder\n",
    "text_folder = '../corpora/tropic_of_orange/texts'\n",
    "\n",
    "\n",
    "# loop over the filename column in df_tropic...      \n",
    "\n",
    "    # print filename\n",
    "    \n",
    "\n",
    "    # get full path\n",
    "    \n",
    "    \n",
    "    # open text\n",
    "    \n",
    "        \n",
    "    # call one of the NER functions and get back the list of results\n",
    "    \n",
    "    \n",
    "    # for each NER result dictionary\n",
    "    \n",
    "        # add the filename to the result dictionary\n",
    "        \n",
    "        # append the result dictionary to all_results\n",
    "        \n",
    "\n",
    "# make a data frame from all of the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Merge the dataframe you just made with df_tropic,\n",
    "# and save the merged dataframe to an excel file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Investigate the counts of 'entity' from the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Filter the dataframe to show only places,\n",
    "# and then investigate the counts of 'entity' from the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Save to an excel file the counts for the place entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining steps:\n",
    "* Upload the excel file of place counts to a Google Drive spreadsheet\n",
    "* Geocode that Google spreadsheet ([see Won-Gi's advice here](https://github.com/quadrismegistus/literarytextmining/issues/2))\n",
    "* Download the excel file to your computer\n",
    "* Open Tableau and connect to the excel file of place *instances*\n",
    "* Then click \"add\" on top left and connect to the excel file of place *counts* (with geocoding)\n",
    "* Select \"fn\" to merge on in Tableau\n",
    "* Click Sheet1 and make a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For geography research team\n",
    "\n",
    "\n",
    "* Map the *non* proper nouns in Tropic of Orange:\n",
    "    * Geocode the nouns:\n",
    "        * Generate list of most frequent nouns in book\n",
    "        * Upload that to a google spreadsheet, share the link with a group of people\n",
    "        * See if you can \"geocode\" these places. Where is Pepsi produced? Where are oranges imported from? Keep a notes column to explain your interpretive decisions!\n",
    "        * Save this spreadsheet to an excel file\n",
    "    * Count the nouns in the texts:\n",
    "        * Get the list of nouns you geocoded from the saved excel file\n",
    "        * Go through the chapters and count those nouns in the chapters\n",
    "        * Package these results into a dataframe of the form:\n",
    "            |fn|word|count|\n",
    "        * Merge that dataframe to the metadata dataframe\n",
    "        * Save the merged form\n",
    "        * Explore the data in Tableau\n",
    "   \n",
    "\n",
    "* See [5C sentiment analysis](5C_sentiment_analysis.ipynb) for a research problem involving sentiment and geography.\n",
    "\n",
    "\n",
    "* Work more with the Tableau file we generated last time and try to answer some research questions.\n",
    "    * Which is the narrator with the greatest geographic range?\n",
    "    * Can we track globalization?\n",
    "    * What are the sentences like that are mentioning far-flung places?\n",
    "    \n",
    "\n",
    "* (Advanced) See if you can figure out how to visualize the networked connections between places in [Palladio](http://hdlab.stanford.edu/palladio-app)\n",
    "    * I've never done this, but I'm pretty sure you'll need to:\n",
    "    * Make a dataframe (and save as CSV) of places, their lat/longs, and their counts\n",
    "    * Make a dataframe (and save as CSV) of places that are mentioned in the same paragraph\n",
    "    * Add both to Palladio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
