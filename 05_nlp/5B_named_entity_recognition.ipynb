{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5B) Named Entity Recognition\n",
    "\n",
    "In this second installment of the NLP module, let's look at a more advanced problem: how can we detect places and people in text?\n",
    "\n",
    "Then, we'll practice mapping geographic patterns in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## How can I identify “named entities” (e.g. proper nouns of places and people)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manzanar loves Named Entities in Chapter 35\n",
    "\n",
    "manzanar = \"\"\"Despite everything, every sports event, concert, and whatnot was happening at the same time. L.A.\n",
    "marathoners slouched by the droves across the finish line at the Coliseum. At the Rose Bowl: UCLA versus\n",
    "USC; the Bruin mascot had been carried off the field with heat stroke, and the Trojan horse was tied up\n",
    "after throwing its sweaty rider. The Clippers were attempting a comeback in overtime at the Sports Arena.\n",
    "It was the end of the seventhinning stretch, and Nomo fans at Chavez Ravine hunkered down with their cold\n",
    "beers and Dodger dogs. Scottie Pippen fouled Shaq who sank a free throw for the Lakers at the Forum in the\n",
    "last seconds. The Trekkie convention warped into five at the L.A. Convention Center. Bud Girls paraded\n",
    "between boxing matches at the Olympic Auditorium. Plácido Domingo belted Rossini at the Dorothy Chandler\n",
    "under the improbable abstract/minimal/baroque direction of Peter Sellers. At the Shrine, executive\n",
    "producer Richard Sakai accepted an Oscar for the movie version of The Simpsons. The helicopter landed for\n",
    "the 944th time on the set of Miss Saigon at the Ahmanson, and Beauty smacked the Beast at the Shubert.\n",
    "Chinese housewives went for the big stakes in pai gow in the Asian room at the Bicycle Club. Live-laughter\n",
    "sitcom audiences and boisterous crowds for the daytime and nighttime talks filled every available studio\n",
    "in Hollywood and Burbank. Thousands of fans melted away with Julio Iglesias at the Universal Amphitheater.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha ha\n",
    "\n",
    "nasa = \"\"\"Ryan Heuser cannot wait until he graduates from Stanford University.\n",
    "He will take up position as Head Engineer of NASA's secret \"Send Literary Critics to Mars\" mission.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some imports\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK\n",
    "\n",
    "We've used NLTK before for word tokenizing, sentence tokenizing, and part of speech tagging. Now let's use it again for named entity recognition using its function `ne_chunk()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Split sentences\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = nltk.sent_tokenize(nasa)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Get words from each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    print(sent_words)\n",
    "    \n",
    "    # empty line\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Get POS for each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    print(sent_pos)\n",
    "    \n",
    "    # empty line\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Get named entity 'chunks' for each sentence\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "    print(chunks)\n",
    "    \n",
    "    # empty line\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Get information about the named entity chunks\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "\n",
    "    # loop over each one\n",
    "    for chunk in chunks:\n",
    "\n",
    "        # if the chunk has a 'label' attribute (is a named entity)\n",
    "        if hasattr(chunk,'label'):\n",
    "\n",
    "            # get the label\n",
    "            label = chunk.label()\n",
    "\n",
    "            # print\n",
    "            print(label)\n",
    "            print(list(chunk))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Get information about the named entity chunks\n",
    "\n",
    "# loop over sentences\n",
    "for sent in sentences:\n",
    "    # get words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    # get POS\n",
    "    sent_pos = nltk.pos_tag(sent_words)\n",
    "    \n",
    "    # get named entity 'chunks'\n",
    "    chunks = nltk.ne_chunk(sent_pos)\n",
    "\n",
    "    # loop over each one\n",
    "    for chunk in chunks:\n",
    "\n",
    "        # if the chunk has a 'label' attribute (is a named entity)\n",
    "        if hasattr(chunk,'label'):\n",
    "\n",
    "            # get the label\n",
    "            label = chunk.label()\n",
    "\n",
    "            # get the words\n",
    "            chunk_words = []\n",
    "            for word,pos in chunk:\n",
    "                chunk_words.append(word)\n",
    "\n",
    "            # make a string version\n",
    "            chunk_words_str = ' '.join(chunk_words)\n",
    "            \n",
    "            print(label,':',chunk_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's make a function for this\n",
    "def ner_nltk(string):\n",
    "    \"\"\"\n",
    "    Using NLTK, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ')\n",
    "    \n",
    "    # sentence tokenize the string\n",
    "    sentences = nltk.sent_tokenize(string)\n",
    "    \n",
    "    # set empty list for output\n",
    "    output_list = []\n",
    "    \n",
    "    # loop over each sentence\n",
    "    sent_num = 0\n",
    "    for sent in sentences:\n",
    "        # add 1 to sent num\n",
    "        sent_num+=1\n",
    "        \n",
    "        # default this to False (see why below)\n",
    "        added_sent_already = False\n",
    "        \n",
    "        # we need to get the words\n",
    "        sent_words = nltk.word_tokenize(sent)\n",
    "        \n",
    "        # parts of speech\n",
    "        sent_pos = nltk.pos_tag(sent_words)\n",
    "        \n",
    "        # then \"chunk\"\n",
    "        chunks = nltk.ne_chunk(sent_pos)\n",
    "        \n",
    "        # loop over chunks...\n",
    "        for chunk in chunks:\n",
    "            # if the chunk has a 'label' attribute (is a named entity)\n",
    "            if hasattr(chunk,'label'):\n",
    "                \n",
    "                # get the label\n",
    "                label = chunk.label()\n",
    "                \n",
    "                # get the words in the chunk\n",
    "                chunk_words = []\n",
    "                for word,pos in chunk:\n",
    "                    chunk_words.append(word)\n",
    "                \n",
    "                # make a string version\n",
    "                chunk_words_str = ' '.join(chunk_words)\n",
    "                \n",
    "                # make a result dictionary\n",
    "                result_dict = {}\n",
    "                \n",
    "                # add NER info\n",
    "                result_dict['type'] = label\n",
    "                result_dict['entity'] = chunk_words_str\n",
    "                \n",
    "                ### optional: add sent info\n",
    "                result_dict['_sent_num'] = sent_num\n",
    "                # add a string of the sentence, but only once per sentence\n",
    "                if not added_sent_already:\n",
    "                    result_dict['_sent'] = sent\n",
    "                    added_sent_already = True\n",
    "                else:\n",
    "                    result_dict['_sent'] = ''\n",
    "                ###\n",
    "                \n",
    "                # add result dictionary to output list\n",
    "                output_list.append(result_dict)\n",
    "    \n",
    "    # return list of dictionaries\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ner_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_ner_ld = ner_nltk(nasa)\n",
    "nltk_ner_ld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_ner_df = pd.DataFrame(nltk_ner_ld)\n",
    "nltk_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_ner_ld = ner_nltk(manzanar)\n",
    "nltk_ner_ld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_ner_df = pd.DataFrame(nltk_ner_ld)\n",
    "nltk_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this data!\n",
    "nltk_ner_df.to_excel('data.ner_nltk.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Polyglot (for non-English, non-French, non-German text)\n",
    "\n",
    "[Polyglot](https://polyglot.readthedocs.io/) is a really cool package, built on top of TextBlob, which supports up to 140 different languages depending on the NLP task. This increase in linguistic range comes at a cost of accuracy, however. The tools was trained using Wikipedia as a Rosetta Stone, calibrating languages' models against each other by using the \"same\" articles in those languages. The other costs of using Polyglot: the documentation isn't that great, and it doesn't seem to be actively updated.\n",
    "\n",
    "*Installation is also kind of a pain in the neck.* I recommend installing this only if you are planning to work with non-English, non-French, non-German text. To do so, paste the following into Terminal:\n",
    "\n",
    "    conda install -c conda-forge pyicu\n",
    "    pip install pycld2\n",
    "    pip install morfessor\n",
    "    pip install polyglot\n",
    "    polyglot download LANG:en   # for english\n",
    "    polyglot download LANG:es   # for spanish (optional)\n",
    "    polyglot download LANG:xx   # where xx is the two-letter language code\n",
    "   \n",
    "See [the website](https://polyglot.readthedocs.io/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_polyglot(string):\n",
    "    \"\"\"\n",
    "    Using polyglot, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"    \n",
    "    \n",
    "    # let's try this...\n",
    "    try:\n",
    "        # to use polyglot, import its \"Text\" object:\n",
    "        from polyglot.text import Text\n",
    "    except ImportError:\n",
    "        print('Polyglot not installed! To do so, follow the instructions above.')\n",
    "        return\n",
    "    # from here on we can assume that polyglot is imported\n",
    "    \n",
    "    # wrap that Text object around any string\n",
    "    pg_text = Text(string)\n",
    "\n",
    "    # make an output list\n",
    "    output_list = []\n",
    "    \n",
    "    # get the entities\n",
    "    entities = pg_text.entities\n",
    "    # loop over sentences\n",
    "    sent_num = 0\n",
    "    for sent in pg_text.sentences:\n",
    "        sent_num+=1\n",
    "\n",
    "        # loop over the entities\n",
    "        added_sent_already = False\n",
    "        for ent in sent.entities:\n",
    "            # get the type\n",
    "            ent_type = ent.tag\n",
    "\n",
    "            # get the words\n",
    "            ent_words = list(ent)\n",
    "\n",
    "            # bogus if the first letter of the first word is not alphabetic (not punctuation)\n",
    "            if not ent_words[0][0].isalpha(): continue\n",
    "\n",
    "            # make a string version\n",
    "            ent_words_str = ' '.join(ent_words)\n",
    "\n",
    "            # make a results dict\n",
    "            result_dict = {}\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = str(sent)\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "            \n",
    "            result_dict['type']=ent_type\n",
    "            result_dict['entity']=ent_words_str\n",
    "\n",
    "            # add to output\n",
    "            output_list.append(result_dict)\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(ner_polyglot(nasa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Manzanar's paragraph\n",
    "#pd.DataFrame(ner_polyglot(manzanar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Spacy\n",
    "\n",
    "[Spacy](http://spacy.io) is industrial-strength NLP. It's the fastest, most powerful, and most accurate. It can also work on [several languages besides English](https://spacy.io/models). But it's also kinda ugly and confusing to use. I recommend using this only if you are working on hundreds of texts and feel extremely comfortable with all the things we've been doing so far.\n",
    "\n",
    "To install:\n",
    "\n",
    "    pip install spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "Here's an NER implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy(string):\n",
    "    \"\"\"\n",
    "    Using spacy, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # import spacy\n",
    "        import spacy\n",
    "    except ImportError:\n",
    "        print(\"spacy not installed. Please follow directions above.\")\n",
    "        return\n",
    "\n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "    \n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # create a spacy text object\n",
    "    doc = nlp(string)\n",
    "    \n",
    "    # make an output list\n",
    "    output_list = []\n",
    "\n",
    "    # loop over sentences\n",
    "    sent_num=0\n",
    "    for sent in doc.sents:\n",
    "        sent_num+=1\n",
    "        added_sent_already = False\n",
    "\n",
    "        # loop over sentence's entities\n",
    "        for ent in sent.ents:\n",
    "            \n",
    "            # make a result dict\n",
    "            result_dict = {}\n",
    "            \n",
    "            # set sentence number\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "            \n",
    "            # store text too\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = sent.text\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "            \n",
    "            # get type\n",
    "            result_dict['type'] = ent.label_\n",
    "            \n",
    "            # get entity\n",
    "            result_dict['entity'] = ent.text\n",
    "            \n",
    "            # get start char\n",
    "            result_dict['star_char'] = ent.start_char\n",
    "            \n",
    "            # get end char\n",
    "            result_dict['star_char'] = ent.end_char\n",
    "            \n",
    "            # add result_dict to output_list\n",
    "            output_list.append(result_dict)\n",
    "            \n",
    "    # return output\n",
    "    return output_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ner_spacy(nasa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ner_ld = ner_spacy(manzanar)\n",
    "spacy_ner_ld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ner_df = pd.DataFrame(spacy_ner_ld)\n",
    "spacy_ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results!\n",
    "spacy_ner_df.to_excel('data.ner_spacy.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting counts from the data\n",
    "\n",
    "Use `value_counts()` to count the values in any column of a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts for the column 'type'\n",
    "val_counts_type = nltk_ner_df['type'].value_counts()\n",
    "val_counts_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts for the column 'entity'\n",
    "val_counts_entity = nltk_ner_df['entity'].value_counts()\n",
    "val_counts_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save any of these val_counts as an excel file itself\n",
    "val_counts_entity.to_excel('data.ner_nltk_entity_counts.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also convert these to dictionaries\n",
    "dict(val_counts_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting in multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count a combination of multiple columns, use .groupby() followed by .size()\n",
    "\n",
    "val_counts_entity_type = results_df.groupby(['entity','type']).size()\n",
    "val_counts_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save this to an excel file too\n",
    "val_counts_entity_type.to_excel('data.ner_nltk_entity_type_counts.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can convert this to a dictionary too\n",
    "dict(val_counts_entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "**@TODO: Make a map of all mentioned places in *Tropic of Orange***\n",
    "\n",
    "Follow the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Get the named entities for the entire Tropic of Orange text\n",
    "#\n",
    "\n",
    "# Load the dataframe for Tropic of Orange\n",
    "df_tropic = pd.read_excel('../corpora/tropic_of_orange/metadata.xls')\n",
    "\n",
    "# make an empty list for all results in the book\n",
    "all_results = []\n",
    "\n",
    "# set a variable to the text folder\n",
    "text_folder = '../corpora/tropic_of_orange/texts'\n",
    "\n",
    "\n",
    "# loop over the filename column in df_tropic...        \n",
    "    # print filename\n",
    "    \n",
    "\n",
    "    # get full path\n",
    "    \n",
    "    \n",
    "    # open text\n",
    "    \n",
    "        \n",
    "    # call one of the NER functions and get back the list of results\n",
    "    \n",
    "    \n",
    "    # for each NER result dictionary\n",
    "    \n",
    "        # add the filename to the result dictionary\n",
    "        \n",
    "        # append the result dictionary to all_results\n",
    "        \n",
    "\n",
    "# make a data frame from all of the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Merge the dataframe you just made with df_tropic,\n",
    "# and save the merged dataframe to an excel file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Investigate the counts of 'entity' from the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Filter the dataframe to show only places,\n",
    "# and then investigate the counts of 'entity' from the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Save to an excel file the counts for the place entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining steps:\n",
    "* Upload the excel file of place counts to a Google Drive spreadsheet\n",
    "* Geocode that Google spreadsheet ([see Won-Gi's advice here](https://github.com/quadrismegistus/literarytextmining/issues/2))\n",
    "* Download the excel file to your computer\n",
    "* Open Tableau and connect to the excel file of place *instances*\n",
    "* Then click \"add\" on top left and connect to the excel file of place *counts* (with geocoding)\n",
    "* Select \"fn\" to merge on in Tableau\n",
    "* Click Sheet1 and make a map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
