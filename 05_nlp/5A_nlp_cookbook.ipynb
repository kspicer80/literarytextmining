{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5A) Natural Language Processing (NLP) Cookbook, part 1\n",
    "\n",
    "So far, our three primary steps of text analysis have consisted of:\n",
    "\n",
    "1. **Reading** the file:\n",
    "    * Filename --> String of entire text\n",
    "2. **Tokenizing** the string:\n",
    "    * String --> List of individual words\n",
    "3. **Counting** the tokens:\n",
    "    * List --> Dictionary of word counts (word as key, count as value)\n",
    "    \n",
    "    \n",
    "Cool. But how can we get more granular information from texts? What about all the other things Natural Language Processing can do, like:\n",
    "\n",
    "* Identify parts of speech? (nouns, verbs, adjectives, ...)\n",
    "* Detect proper nouns of places and people?\n",
    "* Identify sentence boundaries?\n",
    "* Describe sentence syntax? (clauses, subject-object and other \"dependencies\", etc)\n",
    "* Identify noun phrases? (\"natural language processing\", \"education department\", etc)\n",
    "* Sentiment analysis? (estimating positive/negative sentiment of text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to reinvent the wheel. (Although, if you *did* reinvent the wheel, wouldn't that be kind of cool?) There are a number of great software packages out there for natural language processing in Python.\n",
    "\n",
    "This notebook is organized a bit differently. On the top level are basic NLP tasks, and then within each, I'll identify a few different ways to accomplish that NLP task. For instance, how can we tokenize words again? (Perhaps the simplest NLP task.)\n",
    "\n",
    "-----\n",
    "\n",
    "## How can I tokenize a string into a list of words?\n",
    "\n",
    "Let's compare a number of ways to tokenize a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "string = \"We don't need to re-invent the wheel!\"\n",
    "\n",
    "# Spanish\n",
    "string_es = '¡No necesitamos reinventar la rueda!'\n",
    "\n",
    "# German\n",
    "string_de = 'Wir brauchen nicht das Rad neu erfinden!'\n",
    "\n",
    "# French\n",
    "string_fr = \"Nous n'avons pas besoin de réinventer la roue!\"\n",
    "\n",
    "# Chinese (simplified)\n",
    "string_zh = '我们不需要重新发明轮子！'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Just using string's .split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tokenize\"\n",
    "words = string.split()\n",
    "\n",
    "# print words\n",
    "print(words)\n",
    "\n",
    "# print number of words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK\n",
    "\n",
    "This is the one we've worked with so far. NLTK is used most often in academic settings, and by people prototyping algorithms and processing pipelines. Depending on the task, it can support many languages, if you download the appropriate language models.\n",
    "\n",
    "For tokenization, NLTK uses a punctuation-based tokenizer. This should work decently well, but not perfectly, for any language for which spaces and punctuation marks delimit words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk once to use its functions\n",
    "import nltk\n",
    "\n",
    "# tokenize\n",
    "words = nltk.word_tokenize(string)\n",
    "\n",
    "# print words\n",
    "print(words)\n",
    "\n",
    "# print number of words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) TextBlob (recommended)\n",
    "\n",
    "I recommend that we start to move from NLTK to TextBlob, which is built on top of NLTK and [pattern](https://github.com/clips/pattern), and which makes a number of basic NLP tasks extremely easy. TextBlob works for English, French, and German. To install, type in the Terminal:\n",
    "\n",
    "```\n",
    "pip install textblob\n",
    "```\n",
    "\n",
    "For French or German, add:\n",
    "```\n",
    "pip install textblob-fr    # French\n",
    "pip install textblob-de    # German\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob, import the main 'class': TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# create a textblob object\n",
    "blob = TextBlob(string)\n",
    "\n",
    "# print words\n",
    "print(blob.words)\n",
    "\n",
    "# print number of words\n",
    "print(len(blob.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print tokens (words with punctuation)\n",
    "print(blob.tokens)\n",
    "\n",
    "# print number of tokens (words with punctuation)\n",
    "print(len(blob.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Polyglot (for non-English, non-French, non-German text)\n",
    "\n",
    "[Polyglot](https://polyglot.readthedocs.io/) is a really cool package, built on top of TextBlob, which supports up to 140 different languages depending on the NLP task. This increase in linguistic range comes at a cost of accuracy, however. The tools was trained using Wikipedia as a Rosetta Stone, calibrating languages' models against each other by using the \"same\" articles in those languages. The other costs of using Polyglot: the documentation isn't that great, and it doesn't seem to be actively updated.\n",
    "\n",
    "*Installation is also kind of a pain in the neck.* I recommend installing this only if you are planning to work with non-English, non-French, non-German text. To do so, paste the following into Terminal:\n",
    "\n",
    "    conda install -c conda-forge pyicu\n",
    "    pip install pycld2\n",
    "    pip install morfessor\n",
    "    pip install polyglot\n",
    "    polyglot download LANG:en   # for english\n",
    "    polyglot download LANG:es   # for spanish (optional)\n",
    "    polyglot download LANG:xx   # where xx is the two-letter language code\n",
    "   \n",
    "See [the website](https://polyglot.readthedocs.io/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try this...\n",
    "try:\n",
    "    # to use polyglot, import its \"Text\" object:\n",
    "    from polyglot.text import Text\n",
    "\n",
    "    # then wrap that Text object around any string\n",
    "    pg_text = Text(string_es)\n",
    "\n",
    "    # print words\n",
    "    print(pg_text.words)\n",
    "\n",
    "    # print number of words\n",
    "    print(len(pg_text.words))\n",
    "except ImportError:\n",
    "    print('Polyglot not installed! To do so, follow the instructions above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Spacy\n",
    "\n",
    "[Spacy](http://spacy.io) is industrial-strength NLP. It's the fastest, most powerful, and most accurate. It can also work on [several languages besides English](https://spacy.io/models). But it's also kinda ugly and confusing to use. I recommend using this only if you are working on hundreds of texts and feel extremely comfortable with all the things we've been doing so far.\n",
    "\n",
    "To install:\n",
    "\n",
    "    pip install spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "Here's a toy example of spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # import spacy\n",
    "    import spacy\n",
    "\n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # create a spacy text object\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # print words\n",
    "    print(list(doc))\n",
    "\n",
    "    # print number of words\n",
    "    print(len(doc))\n",
    "except ImportError:\n",
    "    print(\"spacy not installed. Please follow directions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: \n",
    "# - Import textblob\n",
    "# - Make a 'blob' from the string (string_arendt) below\n",
    "# - Print the number of words\n",
    "# - Print the number of UNIQUE words\n",
    "# - Calculate the TTR\n",
    "\n",
    "string_arendt = \"\"\"For the sciences today\n",
    "have been forced to adopt a \"language\" of mathematical symbols\n",
    "which, though it was originally meant only as an abbreviation for\n",
    "spoken statements, now contains statements that in no way can be\n",
    "translated back into speech. The reason why it may be wise to\n",
    "distrust the political judgment of scientists qua scientists is not\n",
    "primarily their lack of \"character\" -- that they did not refuse to\n",
    "develop atomic weapons -- or their naivete -- that they did not\n",
    "understand that once these weapons were developed they would\n",
    "be the last to be consulted about their use -- but precisely the fact\n",
    "that they move in a world where speech has lost its power.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I count words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample list of words\n",
    "list_stein = ['a','rose','is','a','rose','is','a','rose','.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Python\n",
    "\n",
    "This was the first way we were doing things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can write our own counter function\n",
    "def count(words):\n",
    "    # make an empty dictionary\n",
    "    dict_of_counts = {}\n",
    "    \n",
    "    # loop over words\n",
    "    for word in words:\n",
    "        # if the word is in the dictionary\n",
    "        if word in dict_of_counts:\n",
    "            # add 1 to its entry\n",
    "            dict_of_counts[word]+=1\n",
    "        else:\n",
    "            # initialize its entry with 1\n",
    "            dict_of_counts[word]=1\n",
    "            \n",
    "    # return dict of counts\n",
    "    return dict_of_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a dictionary of counts using our function\n",
    "count_dict = count(list_stein)\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this will break, because 'daffodil' is not in the count dictionary\n",
    "count_dict['daffodil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will make sure we get a default value (0, in this case) if 'daffodil' is not in the count dictionary\n",
    "count_dict.get('daffodil',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Counter\n",
    "\n",
    "Python includes a very helpful variant on the dictionary called 'Counter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter must be imported\n",
    "from collections import Counter\n",
    "\n",
    "# This is the first way you can make a Counter: just pass it a list of tokens\n",
    "count_dict = Counter(list_stein)\n",
    "\n",
    "# show\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way is to start from an empty one\n",
    "count_dict = Counter()\n",
    "\n",
    "# ...and then loop through and add 1 each time\n",
    "for word in list_stein:\n",
    "    count_dict[word]+=1    # no need to check if it's there!\n",
    "    \n",
    "# show\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A counter will automatically give us a default value of 0 if the word is not there\n",
    "count_dict['daffodil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) textblob\n",
    "\n",
    "The simplest way to count all the words in a text is to just grab the `.word_counts` attribute of a textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, make a blob\n",
    "blob = TextBlob(\"a rose is a rose is a rose.\")\n",
    "\n",
    "# then grab the counter\n",
    "count_dict = blob.word_counts\n",
    "\n",
    "# show (note the lack of period: this is without punctuation)\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will also give us a default value of 0\n",
    "count_dict['daffodil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I tokenize sentences?\n",
    "\n",
    "If word tokenization is the process of splitting an undifferentiated string into a list of words, then sentence tokenization is the process of splitting an undifferentiated string into a list of sentences.\n",
    "\n",
    "Here's a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_labor = \"\"\"Ever since he's been here, never stopped working. Always working.\n",
    "Washing dishes. Chopping vegetables. Cleaning floors. Cooking hamburgers. Painting walls.\n",
    "Laying brick. Cutting hedges. Mowing lawn. Digging ditches. Sweeping trash.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "sentences = nltk.sent_tokenize(para_labor)\n",
    "\n",
    "# print number of sentences\n",
    "print(len(sentences))\n",
    "\n",
    "# show sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) textblob (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a blob\n",
    "blob = TextBlob(para_labor)\n",
    "\n",
    "# Then the sentences are just magically at .sentences\n",
    "sentences = blob.sentences\n",
    "\n",
    "# Print number of sentences\n",
    "print(len(sentences))\n",
    "\n",
    "# Show sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first sentence\n",
    "first_sent = blob.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get that sentence's words\n",
    "first_sent.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can I calculate the average length of sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average words per sentence\n",
    "num_words = len(blob.words)\n",
    "num_sents = len(blob.sentences)\n",
    "wps = num_words / num_sents\n",
    "\n",
    "print(wps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can I calculate the median length of sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the MEDIAN length of sentences\n",
    "\n",
    "# numpy has a lot useful stats functions\n",
    "import numpy as np\n",
    "\n",
    "# set an empty list which we'll use to store the lengths of the sentences\n",
    "sent_lens = []\n",
    "\n",
    "# for every sentence...\n",
    "for sent in blob.sentences:\n",
    "    \n",
    "    # get the number of words\n",
    "    num_sent_words = len(sent.words)\n",
    "        \n",
    "    # add this length to the list of sentence lengths\n",
    "    sent_lens.append(num_sent_words)\n",
    "    \n",
    "# once, we're done looping, print sent_lens\n",
    "print(sent_lens)\n",
    "\n",
    "# print the median\n",
    "median_len = np.median(sent_lens)\n",
    "print(median_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I get the parts of speech?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK\n",
    "\n",
    "English only (by default). NLTK's tagset (as well as textblob's) is drawn from the [Penn Treebank Tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first you need to tokenize\n",
    "words = nltk.word_tokenize(string)\n",
    "\n",
    "# then you can part of speech tag\n",
    "tags = nltk.pos_tag(words)\n",
    "\n",
    "# show tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these is a 'tuple': a baby list with just two things in it\n",
    "babylist = tags[0]\n",
    "babylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means we can get the first thing in this baby list the same way\n",
    "babylist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the second thing\n",
    "babylist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tuple is a 'frozen' list. Nothing can be added to it\n",
    "babylist.append('this will not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can loop over the words and tags like this:\n",
    "for babylist in tags:\n",
    "    word = babylist[0]\n",
    "    tag = babylist[1]\n",
    "    if tag == 'NN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also loop over it like this\n",
    "for word,tag in tags:\n",
    "    # Print if noun\n",
    "    if tag=='NN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) textblob (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a blob\n",
    "blob = TextBlob(string)\n",
    "\n",
    "# then the tags are just magically at .tags\n",
    "tags = blob.tags\n",
    "\n",
    "# show tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over words and tags in the same way as above\n",
    "for word,tag in tags:\n",
    "    # Print if noun\n",
    "    if tag=='NN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) polyglot\n",
    "\n",
    "Polyglot uses a [simplified tag set](https://polyglot.readthedocs.io/en/latest/POS.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # First make a polyglot text\n",
    "    from polyglot.text import Text\n",
    "    text = Text(string_es)\n",
    "\n",
    "    # Then get the tags at .pos_tags\n",
    "    tags = text.pos_tags\n",
    "\n",
    "    # print tags\n",
    "    from pprint import pprint   # this is a prettier print function\n",
    "    pprint(tags)\n",
    "except ImportError:\n",
    "    print('Polyglot not installed. Please see installation instructions above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can loop over these in the same way, too\n",
    "for word,tag in tags:\n",
    "    if tag=='NOUN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) spacy\n",
    "\n",
    "See [here](https://spacy.io/usage/spacy-101)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Count only the nouns in this latest Manzanar chapter (ch35)\n",
    "#\n",
    "\n",
    "# open the file\n",
    "with open('../corpora/tropic_of_orange/texts/ch35.txt') as file:\n",
    "    string_manzanar = file.read()\n",
    "    \n",
    "# make a text blob\n",
    "\n",
    "\n",
    "# make an empty dictionary or counter\n",
    "count_pos = Counter()     # you don't have to check if entry is here\n",
    "\n",
    "\n",
    "# loop over the words and tags\n",
    "# and, if it is a noun,\n",
    "# add 1 to a word's entry in count_nouns \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: Count not words, but parts of speech, in this same chapter\n",
    "# hint: follow the pattern above for the most part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @TODO: \n",
    "# - Make a pandas dataframe for the Tropic of Orange metadata\n",
    "# - For each file in that dataframe,\n",
    "#    - Open that file\n",
    "#    - Read it to a string\n",
    "#    - Make a textblob of it\n",
    "#    - Make a text_results dictionary (remember to include 'fn' as key)\n",
    "#    - Add in the results dictionary:\n",
    "#      - % of words which are nouns (tag starts with \"N\")\n",
    "#      - % of words which are verbs (tag starts with \"V\")\n",
    "#      - % of words which are adjectives (tag starts with \"J\")\n",
    "#    - Store the results dictionaries in a list\n",
    "#    - Make a results dataframe\n",
    "#    - Merge that dataframe to the original metadata frame\n",
    "#    - Make 3 boxplots of nouns/verbs/adjectives by narrator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textblob auf Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the following lines out, but if they don't work, we'll take plan B\n",
    "try:\n",
    "    # import German textblob\n",
    "    from textblob_de import TextBlobDE\n",
    "\n",
    "    # make a German blob\n",
    "    blob_de = TextBlobDE(string_de)\n",
    "\n",
    "    # print words\n",
    "    print(blob_de.words)\n",
    "\n",
    "    # print number of words\n",
    "    print(len(blob_de.words))\n",
    "\n",
    "    # print part of speech tags\n",
    "    print(blob_de.tags)\n",
    "except ImportError:\n",
    "    # here's our Plan B\n",
    "    print('textblob-de not installed. run in terminal: pip install textblob-de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textblob en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the following lines out, but if they don't work, we'll take plan B\n",
    "try:\n",
    "    # import French textblob\n",
    "    from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "    \n",
    "    # make a French blob\n",
    "    blob_fr = TextBlob(string_fr, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "    \n",
    "    # print words\n",
    "    print(blob_fr.words)\n",
    "    \n",
    "    # print number of words\n",
    "    print(len(blob_fr.words))\n",
    "    \n",
    "    # print part of speech tags\n",
    "    print(blob_fr.tags)\n",
    "except ImportError:\n",
    "    # here's our Plan B\n",
    "    print('textblob-fr not installed. run in terminal: pip install textblob-fr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
